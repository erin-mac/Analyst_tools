{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_search = [\"youtube.com\",\"youtu.be\"]\n",
    "pinterest_search = ['pinterest.com','pinimg']\n",
    "reddit_search = ['reddit.com','redd.it']\n",
    "image_search = ['img','png','jpeg','jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run \"/Users/avneeshchandra/Analyst_tools/avneesh/connect_to_api.ipynb\"\n",
    "# %run /Users/avneeshchandra/Analyst_tools/avneesh/load_dependencies.ipynb\n",
    "# %run /Users/avneeshchandra/Analyst_tools/avneesh/graphika_trending.ipynb\n",
    "# # # %run \"/Users/thomaslederer/Documents/graphika_scripts/analysis_tools/get_map_dates.ipynb\"\n",
    "# # # %run \"/Users/thomaslederer/Documents/graphika_scripts/analysis_tools/connect_to_postgres.ipynb\"\n",
    "# # # %run \"/Users/thomaslederer/Documents/graphika_scripts/analysis_tools/get_nodes_data.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns a dataframe with all nodes of a map after fetching their segment data\n",
    "\n",
    "def get_node_data(map_id, grouped = True):\n",
    "    \"\"\"\n",
    "    This function returns all nodes of a given map with their segment data\n",
    "    Inputs:\n",
    "        map_id: Map ID for which to fetch node data\n",
    "        grouped: Whether or not to merge returned df on group data\n",
    "    Outputs:\n",
    "        df0: Dataframe of all nodes in a map with segment data included\n",
    "    \"\"\"\n",
    "    \n",
    "    #This data should also be saved\n",
    "    url = \"https://api.graphika.com/maps/{}/nodes\".format(map_id)\n",
    "    r = re.get(url,auth=(username, pswd))\n",
    "    node_data = r.json()\n",
    "\n",
    "    url = \"https://api.graphika.com/maps/{}/clusters\".format(map_id)\n",
    "    r = re.get(url,auth=(username, pswd))\n",
    "    cluster_data = r.json()\n",
    "\n",
    "\n",
    "    url = \"https://api.graphika.com/maps/{}/groups\".format(map_id)\n",
    "    r = re.get(url,auth=(username, pswd))\n",
    "    group_data = r.json()\n",
    "\n",
    "    df_nodes = pd.DataFrame({\"screen_name\":[n[\"username\"] for n in node_data],\\\n",
    "                            \"node_id\":[n[\"node_source_id\"] for n in node_data],\\\n",
    "                             \"cluster_id\":[n[\"cluster_id\"] for n in node_data]})\n",
    "    df_clusters = pd.DataFrame({\"cluster_id\":[n[\"id\"] for n in cluster_data], \\\n",
    "                                \"cluster_name\": [n[\"name\"] for n in cluster_data],\\\n",
    "                                \"group_id\": [n[\"group_no\"] for n in cluster_data]})\n",
    "    df_clusters[\"cluster_id\"] = df_clusters[\"cluster_id\"].astype(\"int\")\n",
    "    df_group = pd.DataFrame({\"group_id\":[n['group_no'] for n in group_data], \\\n",
    "                            \"group_name\":[n[\"name\"] for n in group_data]})\n",
    "\n",
    "    df0 = pd.merge(df_nodes, df_clusters,on = \"cluster_id\")\n",
    "    \n",
    "    if grouped:\n",
    "    \n",
    "        df0 = pd.merge(df0,df_group, on = \"group_id\")\n",
    "    \n",
    "        df0 = df0[[\"screen_name\",\"node_id\",\"cluster_name\",\"group_name\",\"cluster_id\"]]\n",
    "        \n",
    "        \n",
    "    df0[\"node_id\"] = df0[\"node_id\"].astype(\"str\")\n",
    "\n",
    "    return df0\n",
    "\n",
    "\n",
    "def get_screen_names(map_id):\n",
    "    \"\"\"\n",
    "    This function returns all names of nodes in a given map\n",
    "    Inputs:\n",
    "        map_id: Map ID for which to fetch node data\n",
    "    Outputs:\n",
    "        df0: Dataframe of all nodes in a map with node names included\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://api.graphika.com/maps/%s/nodes\" %map_id\n",
    "    r = re.get(url,auth=(username, pswd))\n",
    "    node_hash = r.json()\n",
    "    screen_names = [x['username'] for x in node_hash]\n",
    "    service_ids = [x[\"service_user_id\"] for x in node_hash]\n",
    "    d = {'screen_name':screen_names,'id':service_ids}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_data(feature, map_ids, use_map_dates = True, date_from = False, date_to = False):\n",
    "    '''This searches within a single map with the option of using the map's creation dates as a param'''\n",
    "    if use_map_dates:\n",
    "        date_from, date_to = get_map_dates(map_ids)\n",
    "    \n",
    "    if type(map_ids) == int:\n",
    "        map_id = map_ids\n",
    "        query = \"SELECT * FROM \"\\\n",
    "        \"(SELECT message_id, hits_twitter_{}.node_id, hit_time, hit_value as hit, map_nodes.map_id, map_nodes.cluster_id FROM hits_twitter_{} \\\n",
    "        join map_nodes on map_nodes.node_id = hits_twitter_{}.node_id) s \\\n",
    "        where s.map_id = {} \\\n",
    "        and s.hit_time BETWEEN '{}'::TIMESTAMP AND '{}'::TIMESTAMP;;\".format(feature, feature, feature, map_id, date_from, date_to)\n",
    "    else:\n",
    "        \n",
    "        query = \"SELECT * FROM \"\\\n",
    "        \"(SELECT message_id, hits_twitter_{}.node_id, hit_time, hit_value as hit, map_nodes.map_id, map_nodes.cluster_id FROM hits_twitter_{} \\\n",
    "        join map_nodes on map_nodes.node_id = hits_twitter_{}.node_id) s \\\n",
    "        where s.map_id in ({}) \\\n",
    "        and s.hit_time BETWEEN '{}'::TIMESTAMP AND '{}'::TIMESTAMP;;\".format(feature, feature, feature, map_ids, date_from, date_to)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print (query)\n",
    "    print (\"getting {} from {} to {}\".format(feature, date_from,date_to))\n",
    "    r = cur.execute(query)\n",
    "    hits = cur.fetchall()\n",
    "    hits_df = pd.DataFrame(hits, columns=[\"message_id\",\"node_id\", \"time\", \"hit_value\", \"map_id\", \"clu_id\"])\n",
    "    hits_df[\"node_id\"] = hits_df[\"node_id\"].astype(\"str\")\n",
    "    hits_df[\"message_id\"] = hits_df[\"message_id\"].astype(\"str\")\n",
    "    hits_df[\"hit_type\"] = feature\n",
    "\n",
    "    return (hits_df)    \n",
    "\n",
    "def pull_data_from_s3 (filename):\n",
    "    '''input the gzip file name'''\n",
    "    s3 = boto.connect_s3()\n",
    "    ma_tweets_bucket = s3.get_bucket('ma-tweets')\n",
    "    results_generator = ma_tweets_bucket.list(\"manual_searches/{}\".format(filename))  \n",
    "    file = [x for x in results_generator]\n",
    "    yr_tweets_gz = file[0].get_contents_as_string()\n",
    "    yr_tweets_json = zlib.decompress(yr_tweets_gz, 16 + zlib.MAX_WBITS)\n",
    "    data = json.loads(yr_tweets_json)\n",
    "    \n",
    "    return (data)\n",
    "\n",
    "def convert_to_df (tweet_data, write_to_file = False, outfile = \"filename\"):    \n",
    "    tweets = []    \n",
    "    for n in tweet_data:\n",
    "\n",
    "        row_entry = {}\n",
    "        row_entry[\"tweet_id\"] = n[\"id_str\"]\n",
    "        row_entry[\"user_id\"] = n[\"user\"][\"id\"]\n",
    "        row_entry[\"screen_name\"] = n[\"user\"][\"screen_name\"]\n",
    "        row_entry[\"time\"] = n ['created_at']\n",
    "        row_entry[\"text\"] = n[\"text\"]\n",
    "\n",
    "        hashtag_dict = (n[\"hashtags\"])\n",
    "        hashtag_list = []\n",
    "        for h in hashtag_dict:\n",
    "            hashtag_list.append(h[\"text\"]) \n",
    "        row_entry[\"hashtags\"] = hashtag_list\n",
    "\n",
    "\n",
    "        if 'retweeted_status' in n.keys():\n",
    "            row_entry[\"retweet\"] = True\n",
    "            row_entry[\"retweet_screen_name\"] = n[\"retweeted_status\"][\"user\"][\"screen_name\"]\n",
    "            row_entry[\"retweet_tweet_id\"] = n[\"retweeted_status\"][\"id_str\"]\n",
    "\n",
    "            if \"retweet_count\" in n[\"retweeted_status\"].keys():\n",
    "                row_entry[\"retweet_count\"] = n[\"retweeted_status\"][\"retweet_count\"]\n",
    "\n",
    "            else: \n",
    "                row_entry[\"retweet_count\"] = \"Missing\"\n",
    "\n",
    "\n",
    "        else:\n",
    "            row_entry[\"retweet\"] = False\n",
    "            row_entry[\"retweet_screen_name\"] = None\n",
    "            row_entry[\"retweet_tweet_id\"] = None\n",
    "            row_entry[\"retweet_count\"] = None\n",
    "\n",
    "\n",
    "\n",
    "        mention_dict = (n[\"user_mentions\"])\n",
    "        mention_list = []\n",
    "        for m in mention_dict:\n",
    "            mention_list.append(m[\"screen_name\"]) \n",
    "        row_entry[\"mentions\"] = mention_list\n",
    "\n",
    "\n",
    "\n",
    "        row_entry[\"language\"] = n['lang']\n",
    "        \n",
    "        url_dict = n[\"urls\"]\n",
    "        url_list = []\n",
    "        for u in url_dict:\n",
    "            url_list.append(u['expanded_url'])\n",
    "        row_entry[\"urls\"] = url_list\n",
    "\n",
    "\n",
    "\n",
    "        tweets.append(row_entry)  \n",
    "\n",
    "\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets)          \n",
    "\n",
    "\n",
    "    if write_to_file:\n",
    "        \n",
    "        writer = pd.ExcelWriter(\"/Users/avneeshchandra/Downloads/{}.xlsx\".format(outfile))\n",
    "        tweets_df.to_excel(writer)\n",
    "        writer.save()\n",
    "\n",
    "    tweets_df[\"tweet_id\"] = tweets_df.tweet_id.astype(str)\n",
    "    return (tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_video_id(url):\n",
    "    t1 = url.find(\"watch?v=\")\n",
    "    t2 = url.find(\"youtu.be/\")\n",
    "    \n",
    "    if t2>t1:\n",
    "        sub_0 = url[t2+9:]  \n",
    "        video_id = sub_0[:12]\n",
    "    \n",
    "    elif t1>t2:\n",
    "        sub_0 = url[t1+8:]  \n",
    "        video_id = sub_0[:11]\n",
    "        \n",
    "    else:\n",
    "        video_id = \"\"  \n",
    "    \n",
    "    return (video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_social_media_links(map_id,from_date = False):\n",
    "\n",
    "    to_date = False\n",
    "    map_dates = True\n",
    "    if from_date != False:\n",
    "        to_date = datetime.now()\n",
    "        map_dates = False\n",
    "        \n",
    "#     print(to_date,from_date,map_dates)\n",
    "\n",
    "    print (\"getting nodes data...\")\n",
    "    nodes = get_node_data(map_id=map_id)\n",
    "#     nodes = pd.DataFrame(sb.Nodes(map_id))\n",
    "\n",
    "    print (\"getting urls data...\")\n",
    "    urls = get_hits_data(map_ids=map_id,feature=\"urls\",use_map_dates = map_dates,date_to = to_date, date_from = from_date)\n",
    "\n",
    "    print (\"getting media data...\")\n",
    "    media = get_hits_data(map_ids=map_id,feature=\"media\",use_map_dates = map_dates,date_to = to_date, date_from = from_date)\n",
    "\n",
    "    df = pd.concat([urls,media])\n",
    "    df.drop_duplicates(\"message_id\", inplace=True)\n",
    "\n",
    "    yt_map_data = pd.merge(df,nodes, left_on=\"node_id\", right_on=\"node_id\")\n",
    "\n",
    "    \n",
    "    return yt_map_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_items (urls_df, target_list):\n",
    "\n",
    "    sm_urls = pd.DataFrame()\n",
    "    for t in target_list:\n",
    "        print (\"filtering for {}\".format(t))\n",
    "        target_urls = urls_df[urls_df.hit_value.str.contains(t)]\n",
    "        if not target_urls.empty:\n",
    "            sm_urls = pd.concat([sm_urls,target_urls])\n",
    "            \n",
    "    sm_urls.reset_index (inplace = True)\n",
    "\n",
    "    return (sm_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistics \n",
    "\n",
    "#statistics \n",
    "\n",
    "def send_youtube_video_request(id_list):\n",
    "    \n",
    "    video_data = []\n",
    "        \n",
    "#     api_url = \"https://www.googleapis.com/youtube/v3/videos?part=snippet&%2CrecordingDetails&\"\n",
    "#     api_url += \"id=%s&\" % \",\".join(id_list)\n",
    "#     api_url += \"key=%s\" % youtube_key\n",
    "    \n",
    "#     response = re.get(api_url)\n",
    "    \n",
    "    SpecificVideoID = \"id=%s&\" % \",\".join(id_list)\n",
    "    SpecificVideoUrl = 'https://www.googleapis.com/youtube/v3/videos?part=snippet%2CcontentDetails%2Cstatistics&'+SpecificVideoID+'&key='+youtube_key\n",
    "#     SpecificVideoUrl = 'https://www.googleapis.com/youtube/v3/channels?part=statistics&id='+SpecificVideoID+'&key='+youtube_key\n",
    "\n",
    "    response = re.get(SpecificVideoUrl) #makes the call to a specific YouTube\n",
    "    # videos = json.load(response) #decodes the response so we can work with it\n",
    "    videos = response.json()\n",
    "    videoMetadata = [] #declaring our list\n",
    "#     for video in videos['items']: \n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        \n",
    "#         search_results = json.loads(response.content)\n",
    "#         return videos\n",
    "#         display(videos)\n",
    "        for x in videos[\"items\"]:\n",
    "            video_dict = {}\n",
    "            video_dict[\"id\"] = (x[\"id\"])\n",
    "            video_dict[\"title\"] = (x['snippet'][\"title\"])\n",
    "            video_dict[\"channel_name\"] = (x['snippet']['channelTitle'])\n",
    "            video_dict['publishedAt'] = x['snippet']['publishedAt']\n",
    "            try:\n",
    "                video_dict['description'] = x['snippet']['description']\n",
    "                video_dict['viewCount'] = x['statistics']['viewCount']\n",
    "                video_dict['likeCount'] = x['statistics']['likeCount']\n",
    "                video_dict['dislikeCount'] = x['statistics']['dislikeCount']\n",
    "                video_dict['favoriteCount'] = x['statistics']['favoriteCount']\n",
    "                video_dict['commentCount'] = x['statistics']['commentCount']\n",
    "            except:\n",
    "                video_dict['description'] = ''\n",
    "                video_dict['viewCount'] = 0\n",
    "                video_dict['dislikeCount'] = 0\n",
    "                video_dict['favoriteCount'] = 0\n",
    "                video_dict['commentCount'] = 0\n",
    "            video_data.append(video_dict)\n",
    "    \n",
    "    return (video_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-e99a72aa07a8>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-e99a72aa07a8>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    SpecificVideoUrl = 'https://www.googleapis.com/youtube/v3/channels?part=statistics&id='&key'&key'=youtube_key\u001b[0m\n\u001b[0m                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# def send_youtube_video_request(id_list):\n",
    "    \n",
    "#     video_data = []\n",
    "        \n",
    "# #     api_url = \"https://www.googleapis.com/youtube/v3/videos?part=snippet&%2CrecordingDetails&\"\n",
    "# #     api_url += \"id=%s&\" % \",\".join(id_list)\n",
    "# #     api_url += \"key=%s\" % youtube_key\n",
    "    \n",
    "# #     response = re.get(api_url)\n",
    "    \n",
    "#     SpecificVideoID = \"id=%s&\" % \",\".join(id_list)\n",
    "#     SpecificVideoUrl = 'https://www.googleapis.com/youtube/v3/videos?part=snippet%2CcontentDetails%2Cstatistics&'+SpecificVideoID+'&key='+youtube_key\n",
    "#     response = re.get(SpecificVideoUrl) #makes the call to a specific YouTube\n",
    "#     # videos = json.load(response) #decodes the response so we can work with it\n",
    "#     videos = response.json()\n",
    "#     videoMetadata = [] #declaring our list\n",
    "# #     for video in videos['items']: \n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "        \n",
    "# #         search_results = json.loads(response.content)\n",
    "# #         return search_results\n",
    "#         for x in videos[\"items\"]:\n",
    "#             video_dict = {}\n",
    "#             video_dict[\"id\"] = (x[\"id\"])\n",
    "#             video_dict[\"title\"] = (x['snippet'][\"title\"])\n",
    "#             video_dict[\"channel_name\"] = (x['snippet']['channelTitle'])\n",
    "#             video_dict['publishedAt'] = x['snippet']['publishedAt']\n",
    "#             try:\n",
    "#                 video_dict['description'] = x['snippet']['description']\n",
    "#                 video_dict['viewCount'] = x['statistics']['viewCount']\n",
    "#                 video_dict['likeCount'] = x['statistics']['likeCount']\n",
    "#                 video_dict['dislikeCount'] = x['statistics']['dislikeCount']\n",
    "#                 video_dict['favoriteCount'] = x['statistics']['favoriteCount']\n",
    "#                 video_dict['commentCount'] = x['statistics']['commentCount']\n",
    "#             except:\n",
    "#                 video_dict['description'] = ''\n",
    "#                 video_dict['viewCount'] = 0\n",
    "#                 video_dict['dislikeCount'] = 0\n",
    "#                 video_dict['favoriteCount'] = 0\n",
    "#                 video_dict['commentCount'] = 0\n",
    "#             video_data.append(video_dict)\n",
    "    \n",
    "#     return (video_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_list(yt_links):\n",
    "    \n",
    "    \n",
    "    video_list = list(set(yt_links.u_id))\n",
    "    video_list = [x for x in video_list if x]\n",
    "    \n",
    "    if len(video_list) > 5000:\n",
    "    \n",
    "    \n",
    "        count_vids = pd.DataFrame(yt_links.groupby(\"u_id\").count().iloc[:,1].sort_values(ascending = False))\n",
    "        video_list = list(count_vids.index[0:5000])\n",
    "        video_list = [x for x in video_list if x]\n",
    "    \n",
    "    return video_list\n",
    "\n",
    "\n",
    "def get_channels (top_videos):\n",
    "#     print (\"sending {} channels to youtube api\".format(len(top_videos)))\n",
    "    saved_data = []\n",
    "\n",
    "    counter = 0\n",
    "    start = 0 \n",
    "    increase = 10\n",
    "\n",
    "    no_groups = math.floor(len(top_videos)/increase)\n",
    "\n",
    "\n",
    "\n",
    "    while counter <= no_groups:\n",
    "\n",
    "        video_input = top_videos[start:start + increase]\n",
    "#         print (\"sending {} to youtube API\".format(video_input))\n",
    "        video_data = send_youtube_video_request(video_input)\n",
    "\n",
    "        if video_data:\n",
    "\n",
    "\n",
    "            saved_data.extend(video_data)\n",
    "        start += increase\n",
    "        counter += 1\n",
    "\n",
    "    video_input = top_videos[start:]\n",
    "    video_data = send_youtube_video_request(video_input)\n",
    "    if video_data:\n",
    "        saved_data.extend(video_data)        \n",
    "        \n",
    "    return (saved_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_filter(df,daysago = 10):\n",
    "    df.time = pd.to_datetime(df.time)\n",
    "    end = datetime.now()\n",
    "    start = datetime.now() - timedelta(days = 10)\n",
    "#     end = '2020-07-08'\n",
    "    mask = (df.time > start) & (df.time <= end)\n",
    "    return df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtube_report(map_id,yt_links = None,num_days = 1):\n",
    "    \n",
    "    if yt_links is None:\n",
    "        print('...Fetching hits')\n",
    "        urls = pd.DataFrame()\n",
    "        if type(map_id) == list:\n",
    "            for an_id in map_id:\n",
    "                try:\n",
    "                    urls = urls.append(get_social_media_links(an_id,datetime.now() - timedelta(days = num_days)))\n",
    "                except:\n",
    "                    continue\n",
    "#             urls = urls.drop_duplicates('hit_value')\n",
    "#             urls = urls.drop_duplicates('message_id')\n",
    "        else:\n",
    "            urls = get_social_media_links(map_id,datetime.now() - timedelta(days = num_days))\n",
    "        yt_links = contains_items(urls,youtube_search)\n",
    "    #     return yt_links\n",
    "        yt_links = date_filter(yt_links)\n",
    "    #     DEBUGGING\n",
    "    ####\n",
    "#         display(yt_links)\n",
    "        yt_links = apply_tags(yt_links.drop(columns = 'cluster_id').rename(columns = {'clu_id':'cluster_id'}))\n",
    "#         yt_links = yt_links[yt_links.tag.str.contains('Health')]\n",
    "        \n",
    "#         return yt_links\n",
    "    ####\n",
    "#         yt_links = yt_links[:10000]\n",
    "    \n",
    "    u_id_df = pd.DataFrame(list(zip(yt_links.hit_value.tolist(), yt_links.hit_value.apply(parse_video_id).values.tolist())))\n",
    "#     return yt_links\n",
    "    yt_links = yt_links.merge(u_id_df,left_on = 'hit_value',right_on = 0).rename(columns = {1:'u_id'}).drop(columns = 0)\n",
    "\n",
    "#     return yt_links\n",
    "#     yt_links[\"u_id\"] = yt_links.hit_value.apply(parse_video_id)\n",
    "#     videos_list = get_videos_list(yt_links[yt_links.tag.str.contains('Health')].drop_duplicates('u_id')[:10000])\n",
    "    videos_list = get_videos_list(yt_links.drop_duplicates('u_id'))\n",
    "    chan_details = get_channels(videos_list)\n",
    "    chan_details = pd.DataFrame(chan_details)\n",
    "#     return yt_links,chan_details\n",
    "#     chan_details = chan_details.drop(columns = 'description')\n",
    "    \n",
    "    yt_links[\"video_id\"] = True\n",
    "    yt_links.loc[yt_links.u_id == \"\",\"video_id\"] = False\n",
    "    yt_links.loc[yt_links.video_id == False,\"u_id\"] = yt_links[\"hit_value\"]\n",
    "\n",
    "    yt_links['date'] = yt_links.time.dt.date\n",
    "#     return yt_links,chan_details,videos_list\n",
    "    recent_shares = pd.pivot_table(data = yt_links,values = 'date',index = 'u_id',aggfunc = pd.Series.max)\n",
    "    recent_shares = recent_shares.rename(columns = {'date':'recent_share_date'})\n",
    "    \n",
    "    \n",
    "    aggby = 'group_name'\n",
    "#     aggby = 'tag'\n",
    "#     aggby = 'cluster_name'\n",
    "    account_shares = pd.pivot_table(data=yt_links, values=\"node_id\", index = \"u_id\", columns=aggby,aggfunc=pd.Series.nunique,fill_value=0)\n",
    "    account_shares[\"total_account_shares\"] = account_shares.sum(axis=1)\n",
    "    account_shares.sort_values(\"total_account_shares\", ascending=False, inplace=True)\n",
    "    account_shares.reset_index(inplace=True)\n",
    "    \n",
    "    account_shares = pd.merge(account_shares,chan_details,left_on=\"u_id\", right_on=\"id\", how=\"outer\")\n",
    "    account_shares.drop(\"id\",axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    tweet_shares = pd.pivot_table(data=yt_links, values=\"message_id\", index = \"u_id\", columns=aggby,aggfunc=pd.Series.nunique,fill_value=0)\n",
    "    tweet_shares[\"total_tweet_shares\"] = tweet_shares.sum(axis=1)\n",
    "    tweet_shares.reset_index(inplace=True)\n",
    "    tweet_shares = tweet_shares[[\"u_id\",\"total_tweet_shares\"]]\n",
    "#     display(account_shares,tweet_shares)\n",
    "    counts = pd.merge(account_shares,tweet_shares)\n",
    "    \n",
    "    \n",
    "    urls_detail = yt_links[[\"u_id\",\"hit_value\"]]\n",
    "    urls_detail.drop_duplicates(\"u_id\", inplace=True)\n",
    "\n",
    "    counts = pd.merge(counts,urls_detail)\n",
    "    counts = counts.merge(recent_shares,right_index = True,left_on = 'u_id')\n",
    "    counts.set_index(\"u_id\",inplace=True)\n",
    "    \n",
    "    cols = list(counts.columns)\n",
    "    cols = ['channel_name','title','hit_value'] + (list(counts.columns))\n",
    "    cols.remove('channel_name')\n",
    "    cols.remove(\"title\")\n",
    "    cols.remove(\"hit_value\")\n",
    "\n",
    "    counts = counts[cols]\n",
    "    \n",
    "    counts = counts.sort_values('total_tweet_shares')\n",
    "\n",
    "    writer = pd.ExcelWriter(\"./YT_report_{}_{}.xlsx\".format(map_id,str(datetime.now())[:10]))\n",
    "\n",
    "#     return counts,yt_links\n",
    "    counts.to_excel(writer,\"counts\")\n",
    "#     yt_links[yt_links.tag.str.contains('Health')].drop_duplicates('u_id')[:10000].to_excel(writer,\"data\")\n",
    "    yt_links.merge(counts.reset_index()[['u_id']],left_on = 'u_id',right_on = 'u_id')\\\n",
    "    .drop_duplicates('u_id')[:10000].to_excel(writer,\"data\")\n",
    "\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube_report([2364],num_days = 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube_report(pd.DataFrame(sb.Maps())[pd.DataFrame(sb.Maps()).is_live == True].id.values.tolist(),yt_links = None,num_days = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtube_pinterest_report(map_id,yt_links = None,num_days = 1):\n",
    "    \n",
    "    if yt_links is None:\n",
    "        print('...Fetching hits')\n",
    "        urls = pd.DataFrame()\n",
    "        if type(map_id) == list:\n",
    "            for an_id in map_id:\n",
    "                urls = urls.append(get_social_media_links(an_id,datetime.now() - timedelta(days = num_days)))\n",
    "            urls = urls.drop_duplicates('message_id')\n",
    "        else:\n",
    "            urls = get_social_media_links(map_id,datetime.now() - timedelta(days = num_days))\n",
    "        yt_links = contains_items(urls,youtube_search)\n",
    "        yt_links = date_filter(yt_links)\n",
    "\n",
    "        yt_links = apply_tags(yt_links.drop(columns = 'cluster_id').rename(columns = {'clu_id':'cluster_id'}))\n",
    "        display(yt_links,yt_links.tag.value_counts())\n",
    "#         yt_links = yt_links[yt_links.tag.str.contains('Health')]\n",
    "    ####\n",
    "#         yt_links = yt_links[:10000]\n",
    "\n",
    "    u_id_df = pd.DataFrame(list(zip(yt_links.hit_value.tolist(), yt_links.hit_value.apply(parse_video_id).values.tolist())))\n",
    "    yt_links = yt_links.merge(u_id_df,left_on = 'hit_value',right_on = 0).rename(columns = {1:'u_id'}).drop(columns = 0)\n",
    "\n",
    "#     yt_links[\"u_id\"] = yt_links.hit_value.apply(parse_video_id)\n",
    "    videos_list = get_videos_list(yt_links[yt_links.tag.str.contains('Health')].drop_duplicates('u_id')[:10000])\n",
    "#     videos_list = get_videos_list(yt_links.drop_duplicates('u_id'))\n",
    "    chan_details = get_channels(videos_list)\n",
    "    chan_details = pd.DataFrame(chan_details)\n",
    "    chan_details = chan_details.drop(columns = 'description')\n",
    "    \n",
    "    yt_links[\"video_id\"] = True\n",
    "    yt_links.loc[yt_links.u_id == \"\",\"video_id\"] = False\n",
    "    yt_links.loc[yt_links.video_id == False,\"u_id\"] = yt_links[\"hit_value\"]\n",
    "\n",
    "    yt_links['date'] = yt_links.time.dt.date\n",
    "#     return yt_links,chan_details,videos_list\n",
    "    recent_shares = pd.pivot_table(data = yt_links,values = 'date',index = 'u_id',aggfunc = pd.Series.max)\n",
    "    recent_shares = recent_shares.rename(columns = {'date':'recent_share_date'})\n",
    "    \n",
    "    account_shares = pd.pivot_table(data=yt_links, values=\"node_id\", index = \"u_id\", columns=\"tag\",aggfunc=pd.Series.nunique,fill_value=0)\n",
    "    account_shares[\"total_account_shares\"] = account_shares.sum(axis=1)\n",
    "    account_shares.sort_values(\"total_account_shares\", ascending=False, inplace=True)\n",
    "    account_shares.reset_index(inplace=True)\n",
    "    \n",
    "    account_shares = pd.merge(account_shares,chan_details,left_on=\"u_id\", right_on=\"id\", how=\"outer\")\n",
    "    account_shares.drop(\"id\",axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    tweet_shares = pd.pivot_table(data=yt_links, values=\"message_id\", index = \"u_id\", columns=\"tag\",aggfunc=pd.Series.nunique,fill_value=0)\n",
    "    tweet_shares[\"total_tweet_shares\"] = tweet_shares.sum(axis=1)\n",
    "    tweet_shares.reset_index(inplace=True)\n",
    "    tweet_shares = tweet_shares[[\"u_id\",\"total_tweet_shares\"]]\n",
    "#     display(account_shares,tweet_shares)\n",
    "    counts = pd.merge(account_shares,tweet_shares)\n",
    "    \n",
    "    \n",
    "    urls_detail = yt_links[[\"u_id\",\"hit_value\"]]\n",
    "    urls_detail.drop_duplicates(\"u_id\", inplace=True)\n",
    "\n",
    "    counts = pd.merge(counts,urls_detail)\n",
    "    counts = counts.merge(recent_shares,right_index = True,left_on = 'u_id')\n",
    "    counts.set_index(\"u_id\",inplace=True)\n",
    "    \n",
    "    cols = list(counts.columns)\n",
    "    cols = ['channel_name','title','hit_value'] + (list(counts.columns))\n",
    "    cols.remove('channel_name')\n",
    "    cols.remove(\"title\")\n",
    "    cols.remove(\"hit_value\")\n",
    "\n",
    "    counts = counts[cols]\n",
    "    \n",
    "    counts = counts.sort_values('recent_share_date')\n",
    "\n",
    "    writer = pd.ExcelWriter(\"./YT_report_{}.xlsx\".format(map_id))\n",
    "\n",
    "#     return counts,yt_links\n",
    "    counts.to_excel(writer,\"counts\")\n",
    "    yt_links[yt_links.tag.str.contains('Health')].drop_duplicates('u_id')[:10000].to_excel(writer,\"data\")\n",
    "#     yt_links.to_excel(writer,\"data\")\n",
    "\n",
    "    writer.save()\n",
    "    return counts,yt_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_temp,y_temp = youtube_pinterest_report([2289,2434,2364],num_days = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_temp,y_temp = youtube_pinterest_report([2719],num_days = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_lookup(rcvd_series):\n",
    "    \"\"\"\n",
    "    This function returns a Series of tags\n",
    "    Inputs:\n",
    "        rcvd_series: An input of cluster IDs\n",
    "    Outputs:\n",
    "        load_tags().loc[map_id,cluster_id]['TAG']: Tag name\n",
    "    \"\"\"\n",
    "    global all_tags\n",
    "    map_id = int(rcvd_series['map_id'])\n",
    "    cluster_id = int(rcvd_series['cluster_no'])\n",
    "#     display(map_id,cluster_id)\n",
    "#     unique_id = str(map_id) + '.' + str(cluster_id)\n",
    "#     display(rcvd_series)\n",
    "    try:\n",
    "        return all_tags.loc[map_id,cluster_id]['tag']\n",
    "    except:\n",
    "        return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_report(map_id,reddit_links = None,num_days = 1):\n",
    "    \n",
    "    if type(reddit_links) == None:\n",
    "        urls = pd.DataFrame()\n",
    "        if type(map_id) == list:\n",
    "            for an_id in map_id:\n",
    "                urls = urls.append(get_social_media_links(an_id,datetime.now() - timedelta(days = num_days)))\n",
    "            urls = urls.drop_duplicates('hit_value')\n",
    "        else:\n",
    "            urls = get_social_media_links(map_id,datetime.now() - timedelta(days = num_days))\n",
    "        reddit_links = contains_items(urls,reddit_search)\n",
    "    #     return yt_links\n",
    "        reddit_links = date_filter(reddit_links)\n",
    "    #     DEBUGGING\n",
    "        reddit_links = reddit_links[:10000]\n",
    "\n",
    "#     pin_links = contains_items(urls,pinterest_search)\n",
    "        \n",
    "\n",
    "    reddit_links[\"reddit_name\"] = parse_subreddits(reddit_links)['name']\n",
    "    reddit_links[\"reddit_type\"] = parse_subreddits(reddit_links)['type']\n",
    "\n",
    "    reddit_links['date'] = reddit_links.time.dt.date\n",
    "    recent_shares = pd.pivot_table(data = reddit_links,values = 'date',index = 'reddit_name',aggfunc = pd.Series.max)\n",
    "    recent_shares = recent_shares.rename(columns = {'date':'recent_share_date'})\n",
    "    \n",
    "    account_shares = pd.pivot_table(data=reddit_links, values=\"node_id\", index = \"reddit_name\", columns=\"group_name\",aggfunc=pd.Series.nunique,fill_value=0)\n",
    "    account_shares[\"total_account_shares\"] = account_shares.sum(axis=1)\n",
    "    account_shares.sort_values(\"total_account_shares\", ascending=False, inplace=True)\n",
    "    account_shares.reset_index(inplace=True)\n",
    "\n",
    "    tweet_shares = pd.pivot_table(data=reddit_links, values=\"message_id\", index = \"reddit_name\", columns=\"group_name\",aggfunc=pd.Series.nunique,fill_value=0)\n",
    "    tweet_shares[\"total_tweet_shares\"] = tweet_shares.sum(axis=1)\n",
    "    tweet_shares.reset_index(inplace=True)\n",
    "    tweet_shares = tweet_shares[[\"reddit_name\",\"total_tweet_shares\"]]\n",
    "#     display(account_shares,tweet_shares)\n",
    "    counts = pd.merge(account_shares,tweet_shares)\n",
    "    \n",
    "    \n",
    "    urls_detail = reddit_links[[\"reddit_name\",\"reddit_type\",\"hit_value\"]]\n",
    "    urls_detail.drop_duplicates(\"reddit_name\", inplace=True)\n",
    "\n",
    "    counts = pd.merge(counts,urls_detail)\n",
    "    counts = counts.merge(recent_shares,right_index = True,left_on = 'reddit_name')\n",
    "    counts.set_index(\"reddit_name\",inplace=True)\n",
    "    \n",
    "    cols = list(counts.columns)\n",
    "    cols = ['reddit_name','reddit_type','hit_value'] + (list(counts.columns))\n",
    "    cols.remove('reddit_name')\n",
    "    cols.remove('reddit_type')\n",
    "    cols.remove(\"hit_value\")\n",
    "\n",
    "    counts = counts[cols]\n",
    "    \n",
    "    counts = counts.sort_values('recent_share_date')\n",
    "\n",
    "    writer = pd.ExcelWriter(\"./reddit_report_{}.xlsx\".format(map_id))\n",
    "\n",
    "    return counts,reddit_links\n",
    "\n",
    "    counts.to_excel(writer,\"counts\")\n",
    "    reddit_links.to_excel(writer,\"data\")\n",
    "\n",
    "    writer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinterest_report(map_id,pt_links = None,num_days = 1):\n",
    "    \n",
    "    if type(pt_links) == None:\n",
    "        urls = pd.DataFrame()\n",
    "        if type(map_id) == list:\n",
    "            for an_id in map_id:\n",
    "                urls = urls.append(get_social_media_links(an_id,datetime.now() - timedelta(days = num_days)))\n",
    "            urls = urls.drop_duplicates('hit_value')\n",
    "        else:\n",
    "            urls = get_social_media_links(map_id,datetime.now() - timedelta(days = num_days))\n",
    "        pt_links = contains_items(urls,pinterest_search)\n",
    "    #     return yt_links\n",
    "        pt_links = date_filter(pt_links)\n",
    "    #     DEBUGGING\n",
    "        pt_links = pt_links[:10000]\n",
    "\n",
    "#     pin_links = contains_items(urls,pinterest_search)\n",
    "        \n",
    "\n",
    "#     pt_links[\"pinterest_name\"] = parse_subreddits(reddit_links)['name']\n",
    "#     pt_links[\"reddit_type\"] = parse_subreddits(reddit_links)['type']\n",
    "\n",
    "    pt_links['date'] = pt_links.time.dt.date\n",
    "    recent_shares = pd.pivot_table(data = pt_links,values = 'date',index = 'hit_value',aggfunc = pd.Series.max)\n",
    "    recent_shares = recent_shares.rename(columns = {'date':'recent_share_date'})\n",
    "    \n",
    "    account_shares = pd.pivot_table(data=pt_links, values=\"node_id\", index = \"hit_value\", columns=\"group_name\",aggfunc=pd.Series.nunique,fill_value=0)\n",
    "    account_shares[\"total_account_shares\"] = account_shares.sum(axis=1)\n",
    "    account_shares.sort_values(\"total_account_shares\", ascending=False, inplace=True)\n",
    "    account_shares.reset_index(inplace=True)\n",
    "\n",
    "    tweet_shares = pd.pivot_table(data=pt_links, values=\"message_id\", index = \"hit_value\", columns=\"group_name\",aggfunc=pd.Series.nunique,fill_value=0)\n",
    "    tweet_shares[\"total_tweet_shares\"] = tweet_shares.sum(axis=1)\n",
    "    tweet_shares.reset_index(inplace=True)\n",
    "    tweet_shares = tweet_shares[[\"hit_value\",\"total_tweet_shares\"]]\n",
    "#     display(account_shares,tweet_shares)\n",
    "    counts = pd.merge(account_shares,tweet_shares)\n",
    "    \n",
    "    \n",
    "    urls_detail = pt_links[[\"hit_value\"]]\n",
    "    urls_detail.drop_duplicates(\"hit_value\", inplace=True)\n",
    "\n",
    "    counts = pd.merge(counts,urls_detail)\n",
    "    counts = counts.merge(recent_shares,right_index = True,left_on = 'hit_value')\n",
    "    counts.set_index(\"hit_value\",inplace=True)\n",
    "    \n",
    "    cols = list(counts.columns)\n",
    "    cols = ['hit_value'] + (list(counts.columns))\n",
    "#     cols.remove('reddit_name')\n",
    "#     cols.remove('reddit_type')\n",
    "    cols.remove(\"hit_value\")\n",
    "\n",
    "    counts = counts[cols]\n",
    "    \n",
    "    counts = counts.sort_values('recent_share_date')\n",
    "\n",
    "    return counts,pt_links\n",
    "\n",
    "    writer = pd.ExcelWriter(\"./pinterest_report_{}.xlsx\".format(map_id))\n",
    "\n",
    "    counts.to_excel(writer,\"counts\")\n",
    "    pt_links.to_excel(writer,\"data\")\n",
    "\n",
    "    writer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_reports(map_id,num_days = 1):\n",
    "    urls = pd.DataFrame()\n",
    "    if type(map_id) == list:\n",
    "        for an_id in map_id:\n",
    "            urls = urls.append(get_social_media_links(an_id,datetime.now() - timedelta(days = num_days)))\n",
    "        urls = urls.drop_duplicates('hit_value')\n",
    "    else:\n",
    "        urls = get_social_media_links(map_id,datetime.now() - timedelta(days = num_days))\n",
    "    pin_links = contains_items(urls,pinterest_search)\n",
    "    reddit_links = contains_items(urls,reddit_search)\n",
    "    youtube_links = contains_items(urls,youtube_search)\n",
    "    \n",
    "    writer = pd.ExcelWriter(\"./domains_report_{}.xlsx\".format(map_id))\n",
    "    \n",
    "    calls = {'pinterest':[pinterest_report,pin_links],'reddit':[reddit_report,reddit_links],'youtube':[youtube_report,youtube_links]}\n",
    "    \n",
    "    for x in ['pinterest','reddit','youtube']:\n",
    "        one,two = calls[x][0](map_id,calls[x][1])\n",
    "        one.to_excel(writer,x + '_counts')\n",
    "        two.to_excel(writer,x + '_data')\n",
    "\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_reports(2364,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_subreddits(df):\n",
    "    link_titles = []\n",
    "    for r in df.hit_value.tolist():\n",
    "        try:\n",
    "            link_titles.append(r.split('/')[1:3])\n",
    "#         print(r.split('/')[1],r)\n",
    "            continue\n",
    "        except:\n",
    "            link_titles.append('Unknown')\n",
    "            continue\n",
    "        link_titles.append('False positive')\n",
    "    return pd.DataFrame(link_titles,columns = ['type','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_date_cells\n",
    "\n",
    "# import xlsxwriter\n",
    "# import string\n",
    "\n",
    "# cell_markers = []\n",
    "# start_cell = 2\n",
    "# end_cell = 0\n",
    "# for d in temp.recent_share_date.astype(str).unique().tolist():\n",
    "#     end_cell = start_cell + temp.recent_share_date.value_counts().loc[d].values[0] - 1\n",
    "#     print('starting at {} and ending at {}'.format(start_cell,end_cell))\n",
    "#     cell_markers.append([start_cell,end_cell])\n",
    "#     start_cell = end_cell + 1\n",
    "    \n",
    "# col_letter = string.ascii_uppercase[temp.columns.tolist().index('recent_share_date')]\n",
    "# cols_to_merge = []\n",
    "# for x in cell_markers:\n",
    "#     cols_to_merge.append([col_letter + str(x[0]),col_letter + str(x[1])])\n",
    "\n",
    "# worksheet = workbook['counts']\n",
    "# for x in cols_to_merge:\n",
    "# #     print('{}:{}'.format(x[0],x[1]))\n",
    "#     worksheet.merge_cells('{}:{}'.format(x[0],x[1]))\n",
    "# workbook.save('/Users/avneeshchandra/Downloads/trial.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_counts (df, feature, count_by_value, map_id = False):\n",
    "        \n",
    "  \n",
    "    s = df.loc[df[feature].str.len() > 0,[feature, count_by_value]]\n",
    "\n",
    "    s1 = s[feature].apply(pd.Series)\\\n",
    "        .merge(s, left_index = True, right_index = True)\\\n",
    "        .drop([feature], axis = 1)\\\n",
    "        .melt(id_vars = [count_by_value], value_name = feature)\\\n",
    "        .drop(\"variable\", axis = 1) \\\n",
    "        .dropna()\n",
    "    \n",
    "    \n",
    "    if map_id:\n",
    "        map_nodes = get_node_data(map_id=map_id)\n",
    "        df.user_id = df.user_id.astype(str)\n",
    "        map_tweets = pd.merge(nodes,df, left_on=\"node_id\",right_on='user_id')\n",
    "    \n",
    "        s1.user_id = s1.user_id.astype(str)\n",
    "\n",
    "        data = pd.merge(s1,map_nodes,left_on=\"user_id\",right_on=\"node_id\")\n",
    "\n",
    "        counts_df = count_by_group(data,feature)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        counts = s1.groupby(feature)[count_by_value].nunique().sort_values(ascending = False)\n",
    "        counts_df = pd.DataFrame(counts)\n",
    "        \n",
    "    counts_df = counts_df[(counts_df.index.str.contains(\"youtube\"))|(counts_df.index.str.contains(\"youtu.be\"))]\n",
    "    return counts_df \n",
    "    \n",
    " \n",
    "        \n",
    "def initial_data_report (filename, filter_by_map = False):\n",
    "    data = pull_data_from_s3(filename)\n",
    "    df = convert_to_df(data) \n",
    "    \n",
    "    if filter_by_map:\n",
    "        counts = get_list_counts(df,\"urls\",'user_id', map_id = filter_by_map)\n",
    "        writer = pd.ExcelWriter(\"/Users/thomaslederer/Downloads/ytlinks_initial{}.xlsx\".format(filter_by_map))\n",
    "\n",
    "       \n",
    "    else:\n",
    "        urls_tweet_counts = get_list_counts(df,\"urls\",'tweet_id')\n",
    "        urls_user_counts = get_list_counts(df,\"urls\",'user_id')\n",
    "        counts = pd.merge(urls_tweet_counts,urls_user_counts,left_index=True, right_index=True)\n",
    "        \n",
    "        d1 = date.today().strftime(\"%Y.%m.%d\")\n",
    "        writer = pd.ExcelWriter(\"/Users/thomaslederer/Downloads/ytlinks_initial{}.xlsx\".format(d1))\n",
    "    \n",
    "    counts.reset_index(inplace=True)\n",
    "    counts[\"u_id\"] = counts.urls.apply(parse_video_id)\n",
    "    channels = get_channels(list(counts[\"u_id\"]))\n",
    "    channels_df = pd.DataFrame(channels)\n",
    "    channels_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    final_df = pd.merge(channels_df,counts, left_on=\"id\",right_on=\"u_id\", how=\"right\")\n",
    "    final_df.sort_values(\"total\", inplace = True, ascending=False)\n",
    "    \n",
    "    \n",
    "    final_df.to_excel(writer)\n",
    "\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_by_group (data, feature):\n",
    "\n",
    "    df = pd.pivot_table(data = data,\n",
    "                index = feature,\n",
    "                columns = \"group_name\",\n",
    "                aggfunc = pd.Series.nunique,\n",
    "                values = \"user_id\",\n",
    "                fill_value = 0)\n",
    "\n",
    "    df[\"total\"] = df.sum(axis = 1)\n",
    "    df.sort_values(by = \"total\",inplace=True, ascending=False)\n",
    "    \n",
    "    return (df)\n",
    "\n",
    "                                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_live_maps (write_to_file = False):\n",
    "    url = \"https://api-production.graphika.io/admin/clustermaps/live\"\n",
    "    r = re.get(url,auth=(username, pswd))\n",
    "    map_data = r.json()\n",
    "    \n",
    "    entries = []\n",
    "    for e in map_data[\"data\"]:\n",
    "        entries.append(e)   \n",
    "    df = pd.DataFrame(entries)\n",
    "    df.drop(\"capture_live_data\", axis=1, inplace = True)\n",
    "    \n",
    "    \n",
    "    now = datetime.now()\n",
    "    date =  str(now)\n",
    "    date = date[:date.find(\" \")]\n",
    "    \n",
    "    \n",
    "    if write_to_file:\n",
    "        filename = \"/Users/thomaslederer/Downloads/%s_live_maps.xlsx\" %date\n",
    "        writer = pd.ExcelWriter(filename)\n",
    "        df.to_excel(writer, index=False)\n",
    "        writer.save()\n",
    "    \n",
    "    return (df)\n",
    "\n",
    "\n",
    "def get_hits_groupby(feature,date_from, date_to,map_ids,groupby=\"clusters\",):\n",
    "    \n",
    "    \n",
    "    #default is to not specify map id and get all live maps\n",
    "    #if not map_ids:\n",
    "    #    live_maps = get_live_maps()\n",
    "    #    map_ids = tuple(live_maps.id.astype(str))\n",
    "#     if len(map_ids) == 1:\n",
    "#         map_ids = '(' + str(map_ids[0]) + ')'\n",
    "\n",
    "    if groupby==\"maps\":\n",
    "        cols = [\"hit\",\"map_id\",\"count\"]\n",
    "        query = \"Select s.hits, s.map_id, count(distinct s.node_id) \\\n",
    "from (select hits_twitter_{}.node_id,hit_value as hits,hit_time,map_id from hits_twitter_{} join map_nodes on map_nodes.node_id = hits_twitter_{}.node_id) \\\n",
    "s  where s.hit_time between '{}' AND '{}' and s.map_id in {} \\\n",
    "group by s.hits, map_id \\\n",
    "order by count(distinct s.node_id) desc;\".format(feature, feature, feature, date_from, date_to,'(' + str(map_ids[0]) + ')')\n",
    "    \n",
    "    if groupby==\"clusters\":\n",
    "        cols = [\"hit\",\"clu_id\",\"map_id\",\"count\"] \n",
    "        query = \"Select s.hits, s.cluster_id,s.map_id, count(distinct s.node_id) \\\n",
    "from (select hits_twitter_{}.node_id,hit_value as hits,hit_time,map_id,cluster_id from hits_twitter_{} join map_nodes on map_nodes.node_id = hits_twitter_{}.node_id) \\\n",
    "s  where s.hit_time between '{}' AND '{}' and s.map_id in {} \\\n",
    "group by s.hits, cluster_id, map_id \\\n",
    "order by count(distinct s.node_id) desc;\".format(feature, feature, feature, date_from, date_to,'(' + str(map_ids[0]) + ')')\n",
    "    \n",
    "    \n",
    "    #print (query)\n",
    "    r = cur.execute(query)\n",
    "    hits = cur.fetchall()\n",
    "    hits_df = pd.DataFrame(hits, columns = cols) \n",
    "    return (hits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supplement_map_name (df):\n",
    "    r = re.get(\"https://api.graphika.com/maps\",auth=(username, pswd))\n",
    "    n_hash = r.json()\n",
    "    map_names = pd.DataFrame(n_hash)\n",
    "    \n",
    "    return_df = pd.merge(df,map_names,right_on=\"id\",left_on=\"map_id\")\n",
    "    return_df.drop([\"id\",\"map_id\"], axis =1, inplace = True)\n",
    "    \n",
    "    return (return_df)\n",
    "\n",
    "\n",
    "def live_report (date_from, date_to):\n",
    "    \n",
    "#     writer = pd.ExcelWriter(f\"/Users/thomaslederer/Downloads/live_map_yt_links_{date_from}{date_to}.xlsx\")\n",
    "    writer = pd.ExcelWriter(f\"/Users/avneeshchandra/Downloads/live_map_yt_links_2643_{date_from}{date_to}.xlsx\")\n",
    "    \n",
    "#     live_maps = get_live_maps()\n",
    "#     live_maps_tuple = tuple(live_maps.id)\n",
    "    live_maps_tuple = tuple([2643])\n",
    "    \n",
    "    urls_groupby = get_hits_groupby(\"urls\",date_from,date_to, map_ids=live_maps_tuple)\n",
    "    media_groupby = get_hits_groupby(\"media\",date_from,date_to, map_ids=live_maps_tuple)\n",
    "#     print (urls_groupby)\n",
    "    urls_groupby = urls_groupby[(urls_groupby.hit.str.contains(\"youtube.com\"))|(urls_groupby.hit.str.contains(\"youtu.be\"))]\n",
    "    media_groupby = media_groupby[(media_groupby.hit.str.contains(\"youtube.com\"))|(media_groupby.hit.str.contains(\"youtu.be\"))]\n",
    "    \n",
    "    media_groupby[\"type\"] = \"media\"\n",
    "    urls_groupby[\"type\"] = \"urls\"\n",
    "    \n",
    "    yt_hits_groupby = pd.concat([media_groupby,urls_groupby])\n",
    "    yt_hits_groupby = supplement_map_name(yt_hits_groupby)\n",
    "#     yt_hits_groupby['name'] = ['mapname' for x in range(yt_hits_groupby.shape[0])]\n",
    "    \n",
    "#     return yt_hits_groupby\n",
    "    yt_pivot = yt_hits_groupby.pivot_table(index=\"hit\", columns=\"name\").fillna(0)\n",
    "\n",
    "    \n",
    "    \n",
    "    media = get_hits_data(\"media\",live_maps_tuple, use_map_dates = False, date_from=date_from, date_to=date_to)\n",
    "    media = media[(media.hit_value.str.contains(\"youtube.com\"))|(media.hit_value.str.contains(\"youtu.be\"))]\n",
    "\n",
    "    urls = get_hits_data(\"urls\",live_maps_tuple, use_map_dates = False, date_from=date_from, date_to=date_to)\n",
    "    urls = urls[(urls.hit_value.str.contains(\"youtube.com\"))|(urls.hit_value.str.contains(\"youtu.be\"))]\n",
    "    \n",
    "    yt_hits = pd.concat([media,urls])\n",
    "\n",
    "    account_totals = pd.DataFrame(yt_hits.groupby(\"hit_value\").nunique(\"node_id\").iloc[:,1].sort_values(ascending = False))\n",
    "    total_shares =  pd.DataFrame(yt_hits.groupby(\"hit_value\").count()[\"message_id\"].sort_values(ascending = False))                                    \n",
    "    totals = pd.merge(account_totals,total_shares,left_index=True, right_index=True)\n",
    "    totals.columns = [\"count_total_users\", \"count_total_shares\"]\n",
    "\n",
    "    df = pd.merge(totals,yt_pivot,right_index=True, left_index=True)\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    df[\"u_id\"] = df[\"index\"].apply(parse_video_id)\n",
    "    channels = get_channels(list(df[\"u_id\"][:5000]))\n",
    "    channels_df = pd.DataFrame(channels)\n",
    "    channels_df.drop_duplicates(inplace=True)\n",
    "#     return [df,channels_df]\n",
    "\n",
    "    final_df = pd.merge(channels_df,df, left_on=\"id\",right_on=\"u_id\", how=\"right\")\n",
    "    #final_df.set_axis(\"index\")\n",
    "    #final_df.index.names = ['url']\n",
    "    #df = df.rename(columns={'node_id': 'unique_accounts', 'message_id': 'total_shares'})\n",
    "\n",
    "\n",
    "\n",
    "    final_df.to_excel(writer)\n",
    "    writer.save()\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
