{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import tweepy\n",
    "import csv\n",
    "from tweepy import Cursor\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "import json\n",
    "import pprint\n",
    "import pathlib\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "                                                                     \n",
    "import pprint as pp\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(pathlib.Path.home().joinpath('api_login.yml'),'r') as fo:\n",
    "    creds = yaml.safe_load(fo)\n",
    "    consumer_key= creds['general']['development']['consumer_key']\n",
    "    consumer_secret = creds['general']['development']['consumer_secret']\n",
    "    access_token = creds['general']['development']['access_token']\n",
    "    access_token_secret = creds['general']['development']['access_token_secret']\n",
    "    youtube_key = creds['youtube']['key']\n",
    "    news_key = creds['news']['key']\n",
    "    botometer_key = creds['botometer']['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twarc import Twarc\n",
    "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# from requests_oauthlib import OAuth1\n",
    "# \n",
    "# \n",
    "# base_twitter_url = \"https://api.twitter.com/1.1/\"\n",
    "# \n",
    "# auth = OAuth1(client_key, client_secret, token, token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets(term: str, map_id = False, grouped: bool = True, limit = False):\n",
    "    \"\"\"\n",
    "    This function uses Tweepy to fetch Twitter data\n",
    "    Inputs:\n",
    "        term: Search term\n",
    "        map_id: Which map to merge data with\n",
    "        grouped: Whether to merge with group data or not\n",
    "        limit: How many 100s of search results to fetch\n",
    "    Outputs:\n",
    "        df: Dataframe of tweets returned from search\n",
    "    \"\"\"    \n",
    "    alltweets = []\n",
    "    new_tweets = api.search(f\"{term}\", count = 100,tweet_mode='extended')\n",
    "    #print (new_tweets)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    #print (alltweets)\n",
    "    #print (alltweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    if alltweets:\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    if limit == False:\n",
    "        limit = 50000\n",
    "    while len(new_tweets) > 0:\n",
    "        if len(alltweets) >= limit:\n",
    "            break\n",
    "        try:\n",
    "            print (\"getting tweets before %s\" %(oldest))\n",
    "        except:\n",
    "            return parse_tweepy_results(alltweets)\n",
    "        #print (alltweets[-1].text)\n",
    "        try:\n",
    "            #all subsequent requests use the max_id param to prevent duplicates\n",
    "            new_tweets = api.search(term, count = 100,max_id=oldest,tweet_mode='extended')\n",
    "            #save most recent tweets\n",
    "            alltweets.extend(new_tweets)\n",
    "\n",
    "            if alltweets:\n",
    "            #update the id of the oldest tweet less one\n",
    "               # print (alltweets)\n",
    "                oldest = alltweets[-1].id - 1\n",
    "\n",
    "            print (\"...%s tweets downloaded so far\"%(len(alltweets)))\n",
    "        except tweepy.TweepError as e:\n",
    "            print(e.reason)\n",
    "            time.sleep(60)\n",
    "\n",
    "    return parse_tweepy_results(alltweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweepy_results(alltweets: dict, map_id = False, grouped: bool = True) -> pd.DataFrame:\n",
    "    tweets_features = []\n",
    "    retweet_user_id = ''\n",
    "    retweet_screen_name = ''\n",
    "    retweet_tweet_id = ''\n",
    "    retweet_count = 0\n",
    "    favorite_count = 0\n",
    "    for tweet in alltweets:\n",
    "        tweet_dict = {}\n",
    "        urls = []\n",
    "        hashtags = []\n",
    "        mentions = []\n",
    "        try:\n",
    "            text_field = tweet.full_text\n",
    "        except:\n",
    "            text_field = tweet.text\n",
    "        if 'quoted_status' in tweet._json.keys():\n",
    "#             print('QT')\n",
    "#             display(tweet)\n",
    "            try:\n",
    "                retweet_text = tweet.quoted_status.full_text\n",
    "                try:\n",
    "                    text = tweet.full_text\n",
    "                except:\n",
    "                    text = tweet.text\n",
    "                retweet = True\n",
    "                retweet_user_id = tweet.quoted_status.user.id_str\n",
    "                retweet_screen_name = tweet.quoted_status.user.screen_name\n",
    "                retweet_tweet_id = tweet.quoted_status.id_str\n",
    "                retweet_count = tweet.retweet_count\n",
    "                favorite_count = tweet.favorite_count\n",
    "                \n",
    "#                 urls = []\n",
    "                for u in tweet.quoted_status.entities[\"urls\"]:\n",
    "                    urls.append(u[\"expanded_url\"])\n",
    "\n",
    "#                 hashtags = []\n",
    "                for h in tweet.quoted_status.entities[\"hashtags\"]:\n",
    "                    hashtags.append(h['text'])\n",
    "\n",
    "#                 mentions = []    \n",
    "                for m in tweet.quoted_status.entities[\"user_mentions\"]:\n",
    "                    mentions.append(m['screen_name'])\n",
    "                \n",
    "                retweet = True\n",
    "                quote_tweet = True\n",
    "            except AttributeError:\n",
    "                retweet = True\n",
    "                quote_tweet = True\n",
    "                text = tweet.full_text#.encode(\"utf-8\")\n",
    "                \n",
    "        \n",
    "        elif str(text_field)[:3] == \"RT \":\n",
    "#             print('RT')\n",
    "#             display(tweet)\n",
    "            try:\n",
    "                try:\n",
    "                    retweet_text = tweet.retweeted_status.full_text\n",
    "                except:\n",
    "                    retweet_text = tweet.retweeted_status.text\n",
    "                try:\n",
    "                    text = tweet.full_text\n",
    "                except:\n",
    "                    text = tweet.text\n",
    "                retweet = True\n",
    "                retweet_user_id = tweet.retweeted_status.user.id_str\n",
    "                retweet_screen_name = tweet.retweeted_status.user.screen_name\n",
    "                retweet_tweet_id = tweet.retweeted_status.id_str\n",
    "                retweet_count = tweet.retweet_count\n",
    "                favorite_count = tweet.favorite_count\n",
    "                \n",
    "#                 urls = []\n",
    "                for u in tweet.retweeted_status.entities[\"urls\"]:\n",
    "                    urls.append(u[\"expanded_url\"])\n",
    "\n",
    "#                 hashtags = []\n",
    "                for h in tweet.retweeted_status.entities[\"hashtags\"]:\n",
    "                    hashtags.append(h['text'])\n",
    "\n",
    "#                 mentions = []    \n",
    "                for m in tweet.retweeted_status.entities[\"user_mentions\"]:\n",
    "                    mentions.append(m['screen_name'])\n",
    "\n",
    "                retweet = True\n",
    "                quote_tweet = False\n",
    "            \n",
    "            except AttributeError:\n",
    "                retweet = True\n",
    "                quote_tweet = False\n",
    "                try:\n",
    "                    text = tweet.full_text#.encode(\"utf-8\")\n",
    "                except:\n",
    "                    text = tweet.text\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                text = tweet.full_text\n",
    "            except:\n",
    "                text = tweet.text#.encode(\"utf-8\") #indented this here\n",
    "            retweet = False\n",
    "            quote_tweet = False\n",
    "            retweet_text = None\n",
    "            retweet_user_id = None\n",
    "            retweet_screen_name = None\n",
    "            retweet_tweet_id = None\n",
    "\n",
    "            retweet_count = tweet.retweet_count\n",
    "            favorite_count = tweet.favorite_count\n",
    "\n",
    "#         urls = []\n",
    "        for u in tweet.entities[\"urls\"]:\n",
    "            urls.append(u[\"expanded_url\"])\n",
    "\n",
    "#         hashtags = []\n",
    "        for h in tweet.entities[\"hashtags\"]:\n",
    "            hashtags.append(h['text'])\n",
    "\n",
    "#         mentions = []    \n",
    "        for m in tweet.entities[\"user_mentions\"]:\n",
    "            mentions.append(m['screen_name'])\n",
    "    \n",
    "        tweet_dict[\"retweet_user_id\"] = retweet_user_id\n",
    "        tweet_dict[\"retweet_screen_name\"] = retweet_screen_name\n",
    "        tweet_dict[\"retweet_tweet_id\"] = retweet_tweet_id \n",
    "        tweet_dict[\"user_id\"] = tweet.user.id_str\n",
    "        tweet_dict[\"retweet\"] = retweet\n",
    "        tweet_dict[\"text\"] = text\n",
    "        tweet_dict['retweet_text'] = retweet_text\n",
    "        tweet_dict[\"author\"] = tweet.author.name\n",
    "        tweet_dict[\"screen_name\"] = tweet.author.screen_name\n",
    "        tweet_dict[\"tweet_id\"] = tweet.id_str\n",
    "        tweet_dict[\"time\"] = tweet.created_at\n",
    "        tweet_dict[\"mentions\"] = mentions\n",
    "        tweet_dict[\"hashtags\"] = hashtags\n",
    "        tweet_dict[\"urls\"] = urls\n",
    "        tweet_dict['retweet_count'] = retweet_count\n",
    "        tweet_dict['favorite_count'] = favorite_count\n",
    "        tweet_dict['source'] = tweet.source\n",
    "        tweet_dict['is_retweet'] = retweet\n",
    "        tweet_dict['is_quote_tweet'] = quote_tweet\n",
    "        tweets_features.append(tweet_dict)\n",
    "\n",
    "        #also need author id so can recreated url, might also be an easier way\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "    # outtweets = [[tweet.author.name, tweet.id_str, tweet.created_at, tweet.full_text.encode(\"utf-8\")] for tweet in alltweets]\n",
    "\n",
    "    df = pd.DataFrame(tweets_features)\n",
    "    \n",
    "    if map_id:\n",
    "        nodes = get_node_data(map_id,grouped)\n",
    "        df = pd.merge(nodes,df)\n",
    "        \n",
    "    print('...Done!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_tweets(tweet_id_list: list):\n",
    "    print('...Getting {} tweets'.format(len(tweet_id_list)))\n",
    "#     tweet_id_list = [1246082527544180736,1246086302178537473]\n",
    "    result_tweets = []\n",
    "    for i in range(0,len(tweet_id_list),100):\n",
    "        try:\n",
    "            result_tweets.extend(api.statuses_lookup(tweet_id_list[i:i+100],tweet_mode = 'extended'))\n",
    "        except:\n",
    "            continue\n",
    "    return parse_tweepy_results(result_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_users(node_id_list: list,field: str = 'id_str'):\n",
    "    node_id_list = [str(x) for x in node_id_list]\n",
    "    wanted_list = node_id_list\n",
    "    filename = '/Users/avneeshchandra/Downloads/all_screennames.csv'\n",
    "    print('...Getting {} users'.format(len(node_id_list)))\n",
    "    screennames_df = stringify_ids(pd.read_csv(filename))\n",
    "    try:\n",
    "        node_id_list = list(set(node_id_list).difference(screennames_df[field].unique().tolist()))\n",
    "    except:\n",
    "        pass\n",
    "    print(f'...Only fetching {len(node_id_list)} new user(s)')\n",
    "    if len(node_id_list) > 0:\n",
    "        result_users = []\n",
    "        for i in range(0,len(node_id_list),100):\n",
    "            try:\n",
    "                if field == 'screen_name':\n",
    "                        result_users.extend(api.lookup_users(screen_names = node_id_list[i:i+100]))\n",
    "                else:\n",
    "                    result_users.extend(api.lookup_users(node_id_list[i:i+100]))\n",
    "            except:\n",
    "                continue\n",
    "        screennames_df = screennames_df.append(stringify_ids(pd.DataFrame([x._json for x in result_users])))\n",
    "        screennames_df.to_csv(filename,index = False)\n",
    "\n",
    "    screennames_df = screennames_df[screennames_df[field].isin(wanted_list)].drop_duplicates('id_str')\n",
    "    return screennames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_members(list_ids: list):\n",
    "    members = []\n",
    "    for i in list_ids:\n",
    "        for page in tweepy.Cursor(api.list_members, list_id = i).items():\n",
    "            members.append(page)\n",
    "    temp_df = pd.DataFrame([x._json for x in members])\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphika_segments(map_id: str = '3108'):\n",
    "    r = re.get(f'http://api.graphika.com/maps/{map_id}/groups', auth=(username, pswd))\n",
    "    groups_df = pd.DataFrame(r.json())\n",
    "    r = re.get(f'http://api.graphika.com/maps/{map_id}/clusters', auth=(username, pswd))\n",
    "    clusters_df = pd.DataFrame(r.json())\n",
    "    segments_df = clusters_df[['name','group_no','cluster_no']].rename(columns = {'name':'cluster_name'})\\\n",
    "        .merge(groups_df[['group_no','name']].rename(columns = {'name':'group_name'}))\n",
    "    segments_df = segments_df[['group_no','group_name','cluster_no','cluster_name']].sort_values('group_no')\n",
    "#     display(segments_df)\n",
    "    return segments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
