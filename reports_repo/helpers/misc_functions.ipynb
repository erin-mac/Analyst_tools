{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set it to None to display all columns/rows in the dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.min_rows',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_ids(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        if 'id' in str(col):\n",
    "            df[col] = df[col].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_cells(s: pd.DataFrame,thresh: int = 0,color_false: str = '',color_true: str = 'black',symbol: str = '✓'):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = []\n",
    "    for i in s:\n",
    "        try:\n",
    "            if i <= thresh:\n",
    "                is_max.append(True)\n",
    "            else:\n",
    "                is_max.append(False)\n",
    "        except:\n",
    "            if i == symbol:\n",
    "                is_max.append(True)\n",
    "            else:\n",
    "                is_max.append(False)\n",
    "    if color_true != '':\n",
    "        color_true = f'background-color: {color_true}'\n",
    "    if color_false != '':\n",
    "        color_false = f'background-color: {color_false}'\n",
    "    print(color_true,color_false)\n",
    "    return [color_true if v else color_false for v in is_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focus_filter(s: pd.DataFrame,thresh: int = 2,symbol: int = '✓'):\n",
    "    '''\n",
    "    highlight focus > thresh in a dataframe\n",
    "    '''\n",
    "    is_max = []\n",
    "    if s.name == 'bridges' or 'Map' in s.name or s.name == 'item':\n",
    "        return s\n",
    "#     print(s.name)\n",
    "    for i in s:\n",
    "        if i >= thresh:\n",
    "            is_max.append(True)\n",
    "        else:\n",
    "            is_max.append(False)\n",
    "#     display(s == 0)\n",
    "#     is_max = s == s.max()\n",
    "    return [symbol if v else '' for v in is_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_csv(df: pd.DataFrame,auto: bool = False):\n",
    "    \"\"\"\n",
    "    This function prints a dataframe\n",
    "    Inputs:\n",
    "        df: Dataframe to print\n",
    "    \"\"\"\n",
    "    df = stringify_ids(df)\n",
    "    filepath = input('Enter folder in your home directory in which to store the file: ')\n",
    "    filename = input('Enter filename (no extension required): ')\n",
    "    try:\n",
    "        filepath = str(pathlib.Path.home()) + '/' + filepath\n",
    "        df.to_csv('{}/{}.csv'.format(filepath + '/',filename))\n",
    "        print('{}.csv stored in {}!'.format(filename,filepath))\n",
    "    except:\n",
    "        print('**Error')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headlines(urls: list):\n",
    "    print('...Getting {} urls'.format(len(urls)))\n",
    "    headlines_df = []\n",
    "    for u in urls:\n",
    "        if 'https://' not in u:\n",
    "            u = 'https://' + u\n",
    "        try:\n",
    "            x = Article(u)\n",
    "            x.download()\n",
    "            x.parse()\n",
    "    #         display(x.title)\n",
    "            headlines_df.append({'urls':u,'headline':x.title})\n",
    "        except:\n",
    "            continue\n",
    "    return pd.DataFrame(headlines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mapnames(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    maps_df = pd.DataFrame(sb.Maps())[['id','name']]\n",
    "    maps_df.columns = ['map_id','map_name']\n",
    "    df = stringify_ids(maps_df).merge(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clusternames(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    map_id = df.map_id.unique()[0]\n",
    "    clusters_df = pd.DataFrame(sb.Clusters(map_id))[['cluster_no','name']]\n",
    "    clusters_df.columns = ['cluster_id','cluster_name']\n",
    "    df = stringify_ids(clusters_df).merge(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_screennames(node_list: list) -> pd.DataFrame:\n",
    "    print(f'Getting {len(node_list)} screennames')\n",
    "    temp = []\n",
    "    for n in node_list:\n",
    "        try:\n",
    "            temp.append({'node_id':n,'screen_name':api.get_user(n)._json['screen_name']})\n",
    "        except:\n",
    "            temp.append({'node_id':n,'screen_name':'Unknown'})\n",
    "    return pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_returner(lst):\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_metadata(tocheck_id: str,map_id: str):\n",
    "    \"\"\"\n",
    "    This function returns metadata of a given node\n",
    "    Inputs:\n",
    "        tocheck_id: Node ID for which to get metadata\n",
    "        map_id: Map to which the node belongs\n",
    "    Outputs:\n",
    "        r.json(): Resulting metadata dataframe for given node\n",
    "    \"\"\"\n",
    "    url = 'https://api.graphika.com/maps/{}/nodes/{}'.format(map_id,tocheck_id)\n",
    "#     print(url)\n",
    "    r = re.get(url, auth = (username,pswd))\n",
    "    try:\n",
    "        return pd.DataFrame([r.json()])\n",
    "    except:\n",
    "        print('**Permission denied for map {}**'.format(map_id))\n",
    "        return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_ids = pd.DataFrame(sb.Maps()).groupby('is_live').get_group(True).id.values.tolist()\n",
    "remove_list = [384,2279]\n",
    "for r in remove_list:\n",
    "            map_ids.remove(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_hitcache(table,value,num_days = 0,to_date = 0,limit = 1000,live = False):\n",
    "#     exact = f'LIKE \\'%{value.lower()}%\\''\n",
    "#     if value != '' and value[0] == value[-1] == \"\\\"\":\n",
    "#         print('...Running an exact search')\n",
    "#         exact = f'= \\'{value[1:-1].lower()}\\''\n",
    "#     if limit != False:\n",
    "#         limit = 'LIMIT ' + str(limit)\n",
    "#     else:\n",
    "#         limit = ''\n",
    "#     if to_date != 0:\n",
    "#         to_date = pd.to_datetime(to_date)\n",
    "#         if type(num_days) == str:\n",
    "#             from_date = num_days\n",
    "#         elif num_days < 1:\n",
    "#             from_date = (to_date-timedelta(hours = 10*num_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#         else:\n",
    "#             from_date = (to_date-timedelta(days = num_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#         to_date = to_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     else:\n",
    "#         to_date = datetime.now()\n",
    "#         if type(num_days) == str:\n",
    "#             from_date = num_days\n",
    "#         elif num_days < 1:\n",
    "#             from_date = (to_date-timedelta(hours = 10*num_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#         else:\n",
    "#             from_date = (to_date-timedelta(days = num_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#         to_date = to_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "#     if live == True:\n",
    "#         map_ids = pd.DataFrame(sb.Maps()).groupby('is_live').get_group(True).id.values.tolist()\n",
    "#         remove_list = [384,2279]\n",
    "#         for r in remove_list:\n",
    "#             map_ids.remove(r)\n",
    "#         map_ids.extend([2802, 2803, 2806, 2808, 2809,2225])\n",
    "#         live = ' AND s.map_id IN ({}) '.format(','.join([str(x) for x in map_ids]))\n",
    "#     elif live == False:\n",
    "#         live = ''\n",
    "#     else:\n",
    "#         live = f' AND s.map_id = {str(live)} '\n",
    "#     query = \"SELECT * FROM         (SELECT * FROM         hits_twitter_{0}         JOIN         map_nodes on map_nodes.node_id = hits_twitter_{0}.node_id         ) s         WHERE         LOWER(s.hit_value) {1}         AND         s.hit_time BETWEEN '{3}'::TIMESTAMP AND '{4}'::TIMESTAMP         {5}         {2};;\".format(table,exact,limit,from_date,to_date,live)\n",
    "# #     display(query)\n",
    "#     r = cur.execute(query)\n",
    "#     hits = cur.fetchall()\n",
    "#     hits_df = pd.DataFrame(hits, columns=[\"id\",\"hit_value\", \"node_id\", \"message_id\",\"hit_time\",\"params\",\"unknown\",\"id_2\",\"node_id2\",\"map_id\",\"cluster_id\"])\n",
    "\n",
    "#     hits_df[\"node_id\"] = hits_df[\"node_id\"].astype(\"str\")\n",
    "#     hits_df[\"message_id\"] = hits_df[\"message_id\"].astype(\"str\")\n",
    "#     hits_df[\"hit_type\"] = table\n",
    "#     hits_df.drop_duplicates('id',inplace = True)\n",
    "#     hits_df.drop(columns = ['unknown','node_id2','id_2','id','params'],inplace = True)\n",
    "\n",
    "#     return hits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hitcache(table: str,\n",
    "                   value: str,\n",
    "                   num_days: int = 0,\n",
    "                   to_date = 0,\n",
    "                   limit: int = 1000,\n",
    "                   live = False,\n",
    "                   tweets_switch: bool = True,\n",
    "                   retweets: bool = False) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function is used to query the hitcache\n",
    "        table: can be hashtags, urls, mentions, tweets, retweets\n",
    "        value: the value to look up – can be a hashtag, URL, mentioned user's ID, tweet ID/user ID, retweet ID\n",
    "        num_days: number of days to look at\n",
    "        to_date: date up to which to search\n",
    "        limit: limit on search results\n",
    "        live: if True, searches all live maps, otherwise also takes individual map IDs\n",
    "        tweets_switch: to toggle the hits_twitter_tweets search between a tweet-based search (False) or a user-based search (True)\n",
    "        retweets: to merge results with the retweets table\n",
    "    '''\n",
    "    field = 'hit_value'\n",
    "    search_value = f'LIKE \\'%{value.lower()}%\\''\n",
    "    if table == 'tweets':\n",
    "        search_value = f'= \\'{value}\\''\n",
    "        if tweets_switch:\n",
    "            field = 'node_id'\n",
    "    if value != '' and value[0] == value[-1] == \"\\\"\":\n",
    "        print('...Running an exact search')\n",
    "        search_value = f'= \\'{value[1:-1].lower()}\\''\n",
    "    if limit != False:\n",
    "        limit = 'LIMIT ' + str(limit)\n",
    "    else:\n",
    "        limit = ''\n",
    "    if to_date != 0:\n",
    "        to_date = pd.to_datetime(to_date)\n",
    "        if type(num_days) == str:\n",
    "            from_date = num_days\n",
    "        elif num_days < 1:\n",
    "            from_date = (to_date-timedelta(hours = 10*num_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            from_date = (to_date-timedelta(days = num_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        to_date = to_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        to_date = datetime.now()\n",
    "        if type(num_days) == str:\n",
    "            from_date = num_days\n",
    "        elif num_days < 1:\n",
    "            from_date = (to_date-timedelta(hours = 10*num_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            from_date = (to_date-timedelta(days = num_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        to_date = to_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    if live == True:\n",
    "        map_ids = pd.DataFrame(sb.Maps()).groupby('is_live').get_group(True).id.values.tolist()\n",
    "#         remove_list = [384,2279]\n",
    "        for r in remove_list:\n",
    "            map_ids.remove(r)\n",
    "#         map_ids.extend([2802, 2803, 2806, 2808, 2809,2225])\n",
    "        live = 'WHERE map_id IN ({}) '.format(','.join([str(x) for x in map_ids]))\n",
    "    elif live == False:\n",
    "        live = ''\n",
    "    else:\n",
    "        live = f'WHERE map_id = {str(live)} '    \n",
    "    if retweets:\n",
    "        if table == 'tweets':\n",
    "            join = f'JOIN (SELECT hit_value AS retweet_message_id, node_id AS retweeter_node_id, message_id AS retweeter_message_id, hit_time AS retweet_hit_time \\\n",
    "                     FROM hits_twitter_retweets) temp ON hits_twitter_{table}.hit_value = temp.retweet_message_id'\n",
    "        else:\n",
    "            join = f'JOIN (SELECT hit_value AS retweet_message_id, node_id AS retweeter_node_id, message_id AS retweeter_message_id, hit_time AS retweet_hit_time \\\n",
    "                     FROM hits_twitter_retweets) temp ON hits_twitter_{table}.message_id = temp.retweet_message_id'\n",
    "    else:\n",
    "        join = ''\n",
    "    sub_query = f'(SELECT * FROM hits_twitter_{table} \\\n",
    "            {join} \\\n",
    "            WHERE LOWER(hits_twitter_{table}.{field}) {search_value} \\\n",
    "            AND hits_twitter_{table}.hit_time BETWEEN \\'{from_date}\\'::TIMESTAMP AND \\'{to_date}\\'::TIMESTAMP \\\n",
    "            {limit}) sub_query'\n",
    "    query = f'SELECT * FROM {sub_query} JOIN map_nodes on map_nodes.node_id = sub_query.node_id {live};;'\n",
    "#     cur = connect_to_hitcache()\n",
    "    conn = connect_to_hitcache()\n",
    "    hits = 0\n",
    "    with conn.cursor() as cur:\n",
    "        r = cur.execute(query)\n",
    "        hits = cur.fetchall()\n",
    "#     cur.close()\n",
    "    cols = [\"id\",\"hit_value\", \"node_id\", \"message_id\",\"hit_time\",\"params\",\"source_id_type\"]\n",
    "    if retweets:\n",
    "        cols.extend([\"retweet_message_id\",'retweeter_node_id','retweeter_message_id','retweet_hit_time'])\n",
    "    cols.extend([\"id\",\"node_id2\",\"map_id\",\"cluster_id\"])\n",
    "    hits_df = pd.DataFrame(hits, columns=cols)\n",
    "    hits_df.drop_duplicates('id',inplace = True)\n",
    "    hits_df = stringify_ids(hits_df)\n",
    "    hits_df[\"hit_type\"] = table\n",
    "    cols_to_drop = ['id','id','params','node_id2','retweet_message_id']\n",
    "    hits_df.drop(columns = cols_to_drop,inplace = True,errors = 'ignore')\n",
    "\n",
    "    hits_df['hit_day'] = pd.to_datetime(hits_df.hit_time.apply(lambda x: str(x)[:10]))\n",
    "    return hits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chronotope(hit_type: str, hit_value: str,days: int,map_id: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Takes a hit type, value, day range, and map and returns a chronotope for it. Saves that to the pwd.\n",
    "    '''\n",
    "    print('...Fetching hits')\n",
    "    temp = check_hitcache(hit_type,hit_value,days,live = map_id, limit = False)\n",
    "    toplot = pd.pivot_table(temp, index = ['map_id','cluster_id','hit_time'],values = 'node_id',aggfunc = pd.Series.nunique).reset_index()\n",
    "    print('...Fetching map segments')\n",
    "    segments_df = graphika_segments(map_id)\n",
    "    segments_df.cluster_no = segments_df.cluster_no.astype(str)\n",
    "    toplot = toplot.merge(segments_df[['cluster_no','group_name','cluster_name']],left_on = 'cluster_id',right_on = 'cluster_no')\n",
    "    toplot['segment'] =  toplot.group_name.values + ',' + toplot.cluster_name.values\n",
    "    print('...Printing chronotope')\n",
    "    for m in toplot.map_id.value_counts().head(5).index.tolist():\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.rcParams['figure.max_open_warning'] = 0\n",
    "        plt.rcParams[\"figure.figsize\"] = (65,50)\n",
    "        # alpha_val = float(x/temp.cluster_id.value_counts().values.max()) for x in \n",
    "    #     s = ax.scatter(toplot[toplot.map_id == m].hit_time, toplot[toplot.map_id == m].cluster_id.astype(int).sort_values().astype(str),\\\n",
    "    #                    c=toplot[toplot.map_id == m].cluster_id.astype(int),cmap = 'binary')\n",
    "        s = ax.scatter(toplot[toplot.map_id == m].hit_time, toplot[toplot.map_id == m].segment.sort_values(),\\\n",
    "                       c=toplot[toplot.map_id == m].cluster_id.astype(int),cmap = 'GnBu',s=300)\n",
    "\n",
    "        fig.suptitle('Map #'+str(m), fontsize=50)\n",
    "        plt.yticks(fontsize=25)\n",
    "        plt.xticks(rotation = 90,fontsize = 30)\n",
    "        plt.colorbar(s)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{hit_value}_chronotope_{m}.jpeg')\n",
    "        plt.show()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_base_report(filepath: str,\n",
    "                       feature_list: list = ['nytimes'],\n",
    "                       feature_type: str = 'mentions',\n",
    "                       substring: bool = True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Search base report for a list of feature values\n",
    "    '''\n",
    "    if feature_type == 'mentions':\n",
    "        feature_type = 'mentioned'\n",
    "    report_df = pd.read_excel(filepath,feature_type.title())\n",
    "#     report_df = pd.read_excel(filepath,feature_type.title())\n",
    "    report_df.item = report_df.item.str.lower()\n",
    "    if substring:\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in feature_list:\n",
    "            res_df = res_df.append(report_df[report_df.item.str.contains(f,case = False)])\n",
    "        return res_df\n",
    "    return report_df[report_df.item.isin([x.lower() for x in feature_list])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_tweets(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    create_temp_table(df.message_id.unique().tolist())\n",
    "    timezone_offset = 6\n",
    "    print('...Fetching retweets')\n",
    "    num_days = 7\n",
    "    pasttime = datetime.now() - timedelta(days = num_days) + timedelta(hours = timezone_offset)\n",
    "    nowtime = datetime.now() + timedelta(hours = timezone_offset)\n",
    "    \n",
    "    conn = connect_to_hitcache()\n",
    "    result = 0\n",
    "    with conn.cursor() as cur:\n",
    "        r = cur.execute(\"SELECT * FROM \\\n",
    "        (SELECT * FROM hits_twitter_retweets WHERE hit_time BETWEEN '{0}'::TIMESTAMP AND '{1}'::TIMESTAMP) \\\n",
    "        as T \\\n",
    "        JOIN temp_table_avneesh ON T.message_id = temp_table_avneesh.tweet_id\".format(pasttime,nowtime))\n",
    "        result = cur.fetchall()\n",
    "    hits_df = parse_hits(result)\n",
    "    print('...Done!')\n",
    "    hits_df = hits_df.rename(columns = {'hit_value':'tweet_id'})\n",
    "    return hits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function returns how many row entries are in which map\n",
    "    Inputs:\n",
    "        df: Dataframe for which to aggregate data\n",
    "    Outputs:\n",
    "        how_many_in_map: Dataframe for which to calculcate map counts and %s\n",
    "    \"\"\"\n",
    "    how_many_in_map = {}\n",
    "    for mid in df.groupby('map_id').groups:\n",
    "        how_many = df.groupby('map_id').get_group(mid).shape[0]\n",
    "        how_many_in_map[mid] = [how_many,100*(how_many/len(df))]\n",
    "        \n",
    "    return pd.DataFrame.from_dict(how_many_in_map,orient='index',columns=['count','percent']).sort_values(['count'],ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_dates(map_id: str):\n",
    "    \"\"\"\n",
    "    This function returns the first and last dates of the data collected in a map\n",
    "    Inputs:\n",
    "        map_id: Map for which to fetch dates\n",
    "    Outputs:\n",
    "        earlier_date: the first date of data collected in the map\n",
    "        later_date: the last date of data collected in the map\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://api.graphika.com/maps/{}\".format(map_id)\n",
    "    r = re.get(url,auth=(username, pswd))\n",
    "    map_data = r.json()\n",
    "    map_dates = map_data[\"data_collection_range\"]\n",
    "    later_date = datetime.strptime(map_dates['end'],'%Y-%m-%dT%H:%M:%S.000Z')\n",
    "    earlier_date = datetime.strptime(map_dates['start'],'%Y-%m-%dT%H:%M:%S.000Z')\n",
    "    return earlier_date,later_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_video_id(url: str):\n",
    "    t1 = url.find(\"watch?v=\")\n",
    "    t2 = url.find(\"youtu.be/\")\n",
    "    \n",
    "    if t2>t1:\n",
    "        sub_0 = url[t2+9:]  \n",
    "        video_id = sub_0[:12]\n",
    "    \n",
    "    elif t1>t2:\n",
    "        sub_0 = url[t1+8:]  \n",
    "        video_id = sub_0[:11]\n",
    "        \n",
    "    else:\n",
    "        video_id = \"\"  \n",
    "    \n",
    "    return video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp_table(a_list,col_name = 'tweet_id'):\n",
    "    recreate_query = 'DROP TABLE IF EXISTS temp_table_avneesh'\n",
    "    create_query = 'CREATE TABLE temp_table_avneesh ({} VARCHAR(25), empty_col INT)'.format(col_name)\n",
    "    populate_query = 'INSERT INTO temp_table_avneesh VALUES {}'.format(','.join('(' + x + ')' for x in a_list))\n",
    "    conn = connect_to_hitcache()\n",
    "#     cur.close()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(recreate_query)\n",
    "        cur.execute(create_query)\n",
    "        cur.execute(populate_query) # no result set returned\n",
    "#    results = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hits(hits: list) -> pd.DataFrame:\n",
    "    if len(hits[0]) == 9:\n",
    "        hits_df = pd.DataFrame(hits, columns=[\"id\",\"hit_value\", \"node_id\", \"message_id\",\"hit_time\",\"params\",\"to_drop\",\"retweet_id\",\"source_type_id\"])\n",
    "    else:\n",
    "        hits_df = pd.DataFrame(hits, columns=[\"id\",\"hit_value\", \"node_id\", \"message_id\",\"hit_time\",\"params\",\"source_type_id\"])\n",
    "    hits_df[\"node_id\"] = hits_df[\"node_id\"].astype(\"str\")\n",
    "    hits_df[\"message_id\"] = hits_df[\"message_id\"].astype(\"str\")\n",
    "    hits_df[\"hit_type\"] = 'hashtags'\n",
    "    hits_df.drop_duplicates('id',inplace = True)\n",
    "    # hits_df.drop(columns = ['node_id2','id_2','id','params'],inplace = True)\n",
    "\n",
    "    return hits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_table(influencer_list: list,mode: str ='influencers'):\n",
    "    influence_list = []\n",
    "#     temp_clusters = pd.read_csv('clusters_API.csv')\n",
    "    temp_clusters = {}\n",
    "    temp_maps = {}\n",
    "#     temp_maps = pd.read_csv('clusters_API.csv')\n",
    "    count = 0\n",
    "    stored_nodes = pd.read_csv('/Users/avneeshchandra/Analyst_tools/avneesh/node_membership.csv')\n",
    "    already_in = stored_nodes.user.unique().tolist()\n",
    "    for node in influencer_list:\n",
    "#         display(count)\n",
    "#         display(temp_clusters)\n",
    "#         display(temp_maps)\n",
    "        try:\n",
    "            name = api.get_user(node)._json['name']\n",
    "            screen_name = api.get_user(node)._json['screen_name']\n",
    "        except:\n",
    "            name = 'Unknown'\n",
    "            screen_name = 'Unknown'\n",
    "        if screen_name in already_in:\n",
    "            continue\n",
    "#         display(HTML('<span style=\"color:black\">Node ID {} {}(@{}) is a top account active in:</span>'.format(node,name,screen_name)))\n",
    "        url = 'https://api.graphika.com/nodes/{}'.format(node)\n",
    "        r = re.get(url,auth = (username,pswd))\n",
    "        response = r.json()[mode]\n",
    "\n",
    "        for k in response:\n",
    "            map_id = k\n",
    "            cluster_id = str(response[k])\n",
    "            \n",
    "            if map_id in temp_maps:\n",
    "                map_name = temp_maps[map_id]\n",
    "                count = count + 1\n",
    "            else:\n",
    "                url = 'https://api.graphika.com/maps/{}'.format(map_id)\n",
    "                r = re.get(url,auth = (username,pswd))\n",
    "                try:\n",
    "                    map_name = str(r.json()['name'])\n",
    "                    temp_maps[map_id] = map_name\n",
    "                    url = 'https://api.graphika.com/maps/{}/clusters'.format(map_id)\n",
    "                    r = re.get(url,auth = (username,pswd))\n",
    "                    for c in r.json():\n",
    "                        temp_clusters[str(map_id) + '.' + str(c['cluster_no'])] = c['name']\n",
    "                except:\n",
    "                    display('Access needed to map {}'.format(map_id))\n",
    "                    temp_maps[map_id] = 'Unknown'\n",
    "                    continue\n",
    "            try:\n",
    "                cluster_name = all_tags.loc[map_id,cluster_id]\n",
    "            except:\n",
    "                check_key = str(map_id) + '.' + str(cluster_id)\n",
    "                if check_key in temp_clusters:\n",
    "#                     print('Found cluster',check_key)\n",
    "                    cluster_name = temp_clusters[check_key]\n",
    "                    count = count + 1\n",
    "                else:\n",
    "#                     print('Could not find cluster',check_key)\n",
    "                    temp_clusters[check_key] = 'Unknown'\n",
    "#                     url = 'https://api.graphika.com/maps/{}/clusters/{}'.format(map_id,cluster_id)\n",
    "#                     r = re.get(url,auth = (username,pswd))\n",
    "#                     try:\n",
    "#                         cluster_name = r.json()['name']\n",
    "#                         temp_clusters[check_key] = cluster_name\n",
    "#                     except:\n",
    "#                         print('Map {} access needed'.format(map_id))\n",
    "#                         temp_clusters[check_key] = 'Unknown'\n",
    "            influence_list.append({'user':screen_name,'map':map_name,'cluster':cluster_name})\n",
    "    print(f'Saved {count} calls')\n",
    "#     temp_clusters.to_csv('clusters_API.csv')\n",
    "#     temp_maps.to_csv('maps_API.csv')\n",
    "    try:\n",
    "        influence_list = pd.DataFrame(influence_list)\n",
    "        influence_list = influence_list.append(stored_nodes)\n",
    "        influence_list = influence_list.set_index('user')\n",
    "        influence_list.to_csv('nodes_table.csv')\n",
    "        return(influence_list)\n",
    "    except:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_name(map_id: str):\n",
    "    \"\"\"\n",
    "    This function returns the name of a map for a given map ID\n",
    "    Inputs:\n",
    "        map_id: Given map ID\n",
    "    Outputs:\n",
    "        r.json()['name']: Map name\n",
    "    \"\"\"\n",
    "    url = 'https://api.graphika.com/maps/{}'.format(map_id)\n",
    "    r = re.get(url,auth=(username,pswd))\n",
    "    try:\n",
    "        return r.json()['name']\n",
    "    except:\n",
    "        print('**Permission denied for map {}**'.format(map_id))\n",
    "        return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_ids(ids_to_check_list: str) -> dict:\n",
    "    \"\"\"\n",
    "    This function returns a dictionary of which maps a given set of nodes appear in\n",
    "    Inputs:\n",
    "        ids_to_check_list: A list of node IDs to check\n",
    "    Outputs:\n",
    "        mapds_dict: A dictionary of node IDs paired with the maps they appear in\n",
    "    \"\"\"\n",
    "    check_for_tweet(ids_to_check_list[0])\n",
    "    maps_dict = {}\n",
    "    for an_id in ids_to_check_list:\n",
    "        maps_dict[an_id] = check_for_tweet(an_id).time\n",
    "\n",
    "    for k in maps_dict.keys():\n",
    "        start_date = maps_dict[k].values[0]\n",
    "        end_date = maps_dict[k].values[0] + np.timedelta64(7,'D')\n",
    "        rtrs = get_rtrs(k)\n",
    "        maps_dict[k] = get_map_counts(get_retweet_data('retweets',k,start_date,end_date))\n",
    "\n",
    "    return maps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_overlap(map_ids: list = [2650,2364]):\n",
    "    nodes = {}\n",
    "    groups = {}\n",
    "    for an_id in map_ids:\n",
    "        nodes[an_id] = pd.DataFrame(sb.Nodes(an_id))\n",
    "        nodes[an_id]['map_id'] = [an_id for x in range(nodes[an_id].shape[0])]\n",
    "        nodes[an_id] = nodes[an_id].dropna()\n",
    "        unique_ids = [str(row[1]['map_id']) + '.' + str(int(row[1]['group_no'])) for row in nodes[an_id].iterrows()]\n",
    "        nodes[an_id]['unique_id'] = unique_ids\n",
    "        \n",
    "        groups[an_id] = pd.DataFrame(sb.Groups(an_id))\n",
    "        unique_ids = [str(an_id) + '.' + str(int(row[1]['group_no'])) for row in groups[an_id].iterrows()]\n",
    "        groups[an_id]['unique_id'] = unique_ids\n",
    "        groups[an_id] = groups[an_id].rename(columns = {'name':'group_name'})\n",
    "        \n",
    "        nodes[an_id] = nodes[an_id].merge(groups[an_id].set_index('unique_id').group_name,on = 'unique_id',how = 'left')\n",
    "        \n",
    "#         display(nodes[an_id])\n",
    "        \n",
    "        print('Map {} has {} nodes across the following groups:'.format(an_id,nodes[an_id].shape[0]))\n",
    "        display(groups[an_id][['group_name','group_no','num_nodes']].set_index('group_name'))\n",
    "        \n",
    "    overlap = nodes[map_ids[0]].merge(nodes[map_ids[1]],on = 'node_source_id',suffixes = ('_'+str(map_ids[0]),'_'+str(map_ids[1])))\n",
    "#     return overlap\n",
    "    crosstab = pd.crosstab(overlap['group_name_'+str(map_ids[0])],overlap['group_name_'+str(map_ids[1])])\n",
    "    \n",
    "    print('{} --> These maps have {} nodes that overlap as follows:'.format(map_ids,overlap.node_source_id.nunique()))\n",
    "    display(crosstab)\n",
    "    return overlap,crosstab\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_graphika_data(df: pd.DataFrame,get_metadata: bool = False):\n",
    "    \"\"\"\n",
    "    This function returns a limited summary of given nodes across Graphika map data\n",
    "    Inputs:\n",
    "        df: An input dataframe of Twitter data\n",
    "        get_metadata: Whether or not to fetch node metadata\n",
    "    Outputs:\n",
    "        Data on how many unique nodes appear in a dataframe, which maps they appear in, and their segment data\n",
    "    \"\"\"\n",
    "    all_nodes = set(df.user_id.tolist())\n",
    "    print('From this search, {} nodes were returned'.format(len(all_nodes)))\n",
    "    result_node_df = pd.DataFrame()\n",
    "\n",
    "    for node_id in all_nodes:\n",
    "        result_node_df = result_node_df.append(get_hit('nodes',node_id,how_many = 10,get_groups = get_metadata,get_clusters = get_metadata,get_nodenames = get_metadata))\n",
    "\n",
    "    map_summary_table = get_map_counts(result_node_df)\n",
    "    top_maps = map_summary_table.index.tolist()[:3]\n",
    "    top_names = [get_map_name(top_maps[0]),get_map_name(top_maps[1]),get_map_name(top_maps[2])]\n",
    "    print('The map that have the most of these nodes are \\n{} \\n{} \\nand {}'.format(top_names[0],top_names[1],top_names[2]))\n",
    "    display(get_map_counts(result_node_df))\n",
    "    \n",
    "    if input('>> Would you like to save a table of which maps these nodes appear in? (y/n) \\n') == 'y':\n",
    "        print_csv(result_node_df.set_index('map_id'))\n",
    "    return result_node_df.set_index('map_id')\n",
    "    # current_df = get_hit('nodes','2972164989',how_many = 10)\n",
    "    # set(current_df.map_id.tolist())\n",
    "    # current_df = newtemp[newtemp.map_id == 1687]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphika_search(q: str ='test',feature_type: str ='all',limit: int =10000,map_id: str ='3108') -> pd.DataFrame:\n",
    "    endpoint = 'https://api.graphika.com/search'\n",
    "    if feature_type == 'all':\n",
    "        feature_type = ['post_url','link_url','media']\n",
    "    else:\n",
    "        feature_type = [feature_type]\n",
    "    temp = []\n",
    "    for f in feature_type:\n",
    "        query = {\n",
    "            \"q\": q,\n",
    "            \"fuzzy_match\": 1,\n",
    "            \"limit\": limit,\n",
    "            \"offset\": 0, \n",
    "            \"insignificant\": 1,\n",
    "            \"feature_types\": feature_type,\n",
    "            \"map_ids\": map_id,\n",
    "            \"sortBy\": \"engagement_count\"\n",
    "        }\n",
    "        result = re.post(endpoint, data = json.dumps(query),auth = (username,pswd))\n",
    "        try:\n",
    "            temp.extend(result.json()['results'])\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        return pd.DataFrame(temp)\n",
    "    except:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starship_links(rcvd_series) -> str:\n",
    "    if 'url' in rcvd_series.hit_type or rcvd_series.hit_type == 'media':\n",
    "        rcvd_series.target = rcvd_series.target.replace('/','%2F').replace('?','%3F').replace('=','%3D')\n",
    "    return f'https://starship.graphika.com/maps/{rcvd_series.map_id}/features/{rcvd_series.hit_type}/{rcvd_series.target}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def narrow_hits(df: pd.DataFrame):\n",
    "#     filters for duplicate hits\n",
    "    df = df.drop_duplicates('message_id')\n",
    "#     filters for possible RTs\n",
    "    df = df.drop_duplicates('hit_time')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustername_lookup(rcvd_series):\n",
    "    \"\"\"\n",
    "    This function returns a Series of cluster names\n",
    "    Inputs:\n",
    "        rcvd_series: An input of cluster IDs\n",
    "    Outputs:\n",
    "        r.json()['name']: Cluster name\n",
    "    \"\"\"\n",
    "#     global user_maps\n",
    "    map_id = rcvd_series['map_id']\n",
    "#     if map_id not in user_maps:\n",
    "#         return 'Access required'\n",
    "    cluster_id = rcvd_series['cluster_id']\n",
    "#     try:\n",
    "#         map_clusters = sb.Clusters(map_id)\n",
    "#     except:\n",
    "#         return '**Map access needed'\n",
    "#     for mc in map_clusters:\n",
    "#         if mc['cluster_no'] == cluster_id:\n",
    "#             return mc['name']\n",
    "#         else:\n",
    "#             continue\n",
    "#     return 'Unknown'\n",
    "    url = 'https://api.graphika.com/maps/{}/clusters/{}'.format(map_id,cluster_id)\n",
    "    r = re.get(url, auth = (username,pswd))\n",
    "    try:\n",
    "        return r.json()['name']\n",
    "    except:\n",
    "        return '**Map access needed**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_changes(df: pd.DataFrame) -> None:\n",
    "    user_lang_df = {}\n",
    "    for lang in df.index.tolist()[:5]:\n",
    "        user_lang_df[lang] = {}\n",
    "        old_count = 0\n",
    "        for yr in df.columns.tolist():\n",
    "            new_count = int(df[yr][lang])\n",
    "            try:\n",
    "                change = int(100*(new_count - old_count)/old_count)\n",
    "                user_lang_df[lang][yr] = change\n",
    "\n",
    "                old_count = new_count\n",
    "            except:\n",
    "                user_lang_df[lang][yr] = '-'\n",
    "                old_count = new_count\n",
    "                continue\n",
    "    display(pd.DataFrame(user_lang_df).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame):\n",
    "    years = [i[:4] for i in df.tweet_time]\n",
    "    df['year'] = years\n",
    "\n",
    "    domains = []\n",
    "    for i in df.urls.values.tolist():\n",
    "        if i == '[]':\n",
    "            domains.append(i)\n",
    "            continue\n",
    "        try:\n",
    "            domains.append(i[2:-2].split('//')[1].split('/')[0])\n",
    "        except:\n",
    "            domains.append('Unknown')\n",
    "#             print(i)\n",
    "\n",
    "    df['domains'] = domains\n",
    "    \n",
    "    df = df.rename(columns = {'user_mentions':'mentions'})\n",
    "    feature_df = {'urls':[],'hashtags':[],'mentions':[]}\n",
    "\n",
    "    for feature in ['urls','hashtags','mentions']:\n",
    "        temp_df = df[~df[feature].isna()]\n",
    "        temp_list = [ast.literal_eval(i) for i in temp_df[temp_df[feature] != '[]'][feature].dropna().values.tolist()]\n",
    "        feature_df[feature] = temp_df[temp_df[feature] != '[]']\n",
    "        feature_df[feature][feature] = temp_list\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annual_tables(df: pd.DataFrame,sort_by: str = 'retweet_count',func = pd.Series.sum):\n",
    "    feature_pivot = {}\n",
    "    for feature in ['urls','hashtags','mentions']:\n",
    "        feature_pivot[feature] = pd.pivot_table(data=df[feature].explode(feature), index=['year',feature], aggfunc=func, values=sort_by).dropna().sort_values(['year',sort_by],ascending = False)\n",
    "        feature_pivot[feature] = pd.concat([d.nlargest(50, columns=[sort_by]) for _,d in feature_pivot[feature].groupby('year')])\n",
    "    feature_pivot['domains'] = pd.pivot_table(data=df['urls'], index=['year','domains'], aggfunc=func, values=sort_by).dropna().sort_values(['year',sort_by],ascending = False)\n",
    "    feature_pivot['domains'] = pd.concat([d.nlargest(50, columns=[sort_by]) for _,d in feature_pivot['domains'].groupby('year')])\n",
    "    \n",
    "    screennames_temp = []\n",
    "    limit_screennames = 0\n",
    "    temp_year_old = 0\n",
    "    for i in feature_pivot['mentions'].index:\n",
    "        limit_screennames = limit_screennames + 1\n",
    "        temp_year_new = i[0]\n",
    "        if temp_year_new != temp_year_old:\n",
    "#             print('Resetting limit',limit_screennames)\n",
    "            limit_screennames = 0\n",
    "        if limit_screennames > 10:\n",
    "            screennames_temp.append('Not a top 10 mention')\n",
    "            continue\n",
    "        try:\n",
    "            screennames_temp.append(api.get_user(i[1]).screen_name)\n",
    "            temp_year_old = i[0]\n",
    "        except:\n",
    "            screennames_temp.append('Suspended')\n",
    "            temp_year_old = i[0]\n",
    "\n",
    "    feature_pivot['mentions']['screen_names'] = screennames_temp\n",
    "    \n",
    "    return feature_pivot\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_annualized_report(pivots,name):\n",
    "    writer = pd.ExcelWriter('./{}_report_annualized.xlsx'.format(name),engine = 'xlsxwriter')\n",
    "    for sheet_name in pivots.keys():\n",
    "        print('...Writing',sheet_name)\n",
    "        try:\n",
    "            pivots[sheet_name].to_excel(writer,sheet_name)\n",
    "        except:\n",
    "            print('...Error')\n",
    "            continue\n",
    "    writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(data: dict,filename: str,eng = 'xlsxwriter',opt={'strings_to_urls': False}):\n",
    "    writer = pd.ExcelWriter(filename,engine = eng, options = opt)\n",
    "    for k in data:\n",
    "        data[k] = stringify_ids(data[k])\n",
    "        data[k].to_excel(writer,k)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_sheet(data: dict,filename: str,update: bool = False,sheet_name: str = 'Sheet1',alert: bool = False,ix: bool = False):\n",
    "    gsheetswriter = Spread(filename,create_spread = not update)\n",
    "    if update:\n",
    "        print('...Updating sheet')\n",
    "        print('...Sheet currently has:')\n",
    "        for x in gsheetswriter.sheets:\n",
    "            print(x,gsheetswriter.get_sheet_dims(x))\n",
    "    else:\n",
    "        print('...Opening Sheet')\n",
    "    for k in data:\n",
    "        data[k] = stringify_ids(data[k])\n",
    "        print(f'...Writing {k}')\n",
    "        if update:\n",
    "            bottom_of_sheet = gsheetswriter.get_sheet_dims(k)[0]\n",
    "        else:\n",
    "            bottom_of_sheet = 0\n",
    "        gsheetswriter.df_to_sheet(data[k], index=ix, sheet=k, start='A{}'.format(bottom_of_sheet+1),\\\n",
    "                                  replace=not update,headers = not update)\n",
    "    gsheetswriter.delete_sheet('Sheet1')\n",
    "    print('...Sheet now has:')\n",
    "    for x in gsheetswriter.sheets:\n",
    "        print(x,gsheetswriter.get_sheet_dims(x))\n",
    "    if alert:\n",
    "        send_to_slack(f'...Sheet available at: {gsheetswriter.url}')\n",
    "    print(f'...Sheet available at: {gsheetswriter.url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_slack(message: str = 'Test',hook: str = 'https://hooks.slack.com/services/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'):\n",
    "    slack = Slack(url=hook)\n",
    "    slack.post(text=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gsheet(filename: str,sheetname: str):\n",
    "    gsheetswriter = Spread(filename,sheet = sheetname,create_spread = False)\n",
    "    temp = gsheetswriter.sheet_to_df(index = False)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_update(filename: str = 'live_monitoring'):\n",
    "    gsheetswriter = Spread(filename,sheet = 'hits',create_spread = False)\n",
    "    temp = gsheetswriter.sheet_to_df(index = False)\n",
    "    return pd.to_datetime(temp.hit_time).sort_values().astype(str).tail(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_filter(df: pd.DataFrame,user: str = 'andrew'):\n",
    "    filters = {'andrew':[1910,2503,2289,2364,2426,2434,2467]}\n",
    "    filtered_df = pd.DataFrame()\n",
    "    for m in filters[user]:\n",
    "        filtered_df = filtered_df.append(df[df.map_id.str.contains(str(m))])\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hydrated_tweets(tweet_id_list: list) -> list:\n",
    "    hydrated_tweets_list = []\n",
    "    for twt in t.hydrate(tweet_id_list):\n",
    "        hydrated_tweets_list.append(twt)\n",
    "    return hydrated_tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_engagement(url_list: list) -> pd.DataFrame:\n",
    "    popular_tweets = pd.DataFrame()\n",
    "    engagement_df = []\n",
    "    for a_url in url_list:\n",
    "        temp_url_df = []\n",
    "        for result in api.search(q = 'url:{}'.format(a_url), result_type = 'popular'):\n",
    "            temp_url_df.append(result._json)\n",
    "        if temp_url_df == []: continue\n",
    "        temp_url_df = pd.DataFrame(temp_url_df)\n",
    "        popular_tweets = popular_tweets.append(temp_url_df)\n",
    "#         display(temp_url_df)\n",
    "        row = {'url':a_url,'total retweets':temp_url_df.retweet_count.sum(),'total favorites':temp_url_df.favorite_count.sum()}\n",
    "        engagement_df.append(row)\n",
    "    return pd.DataFrame(engagement_df).set_index('url').rename(columns = {'total retweets':'source_retweet_count','total favorites':'source_favorite_count'})#,popular_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphika engagement\n",
    "\n",
    "def get_twitter_engagement(df: pd.DataFrame):\n",
    "    twitter_engagement = pd.DataFrame()\n",
    "    for top_url in df.index.value_counts().index.unique().tolist():\n",
    "        top_url_temp = df[df.index.str.contains(top_url)]\n",
    "\n",
    "        #First order finding of RT counts\n",
    "        top_url_tweets = get_hydrated_tweets(top_url_temp.message_id.value_counts().index.values.tolist())\n",
    "        top_url_tweets = pd.DataFrame(top_url_tweets)\n",
    "    #     display(top_url_tweets)\n",
    "        top_url_retweets_ids = []\n",
    "        #Second order finding of RT counts\n",
    "        for row in top_url_tweets.fillna('-').iterrows():\n",
    "            try:\n",
    "                if not row[1]['retweeted_status'] == '-':\n",
    "                    top_url_retweets_ids.append(row[1]['retweeted_status']['id_str'])\n",
    "                if not row[1]['quoted_status'] == '-':\n",
    "                    top_url_retweets_ids.append(row[1]['quoted_status']['id_str'])\n",
    "            except:\n",
    "                continue\n",
    "        top_url_retweets = get_hydrated_tweets(top_url_retweets_ids)\n",
    "\n",
    "        temp_engagement = top_url_tweets.append(pd.DataFrame(top_url_retweets))\n",
    "        temp_engagement['hit_value'] = [top_url for x in range(temp_engagement.shape[0])]\n",
    "        twitter_engagement = twitter_engagement.append(temp_engagement)\n",
    "    # pd.DataFrame(hit_tweets).loc[0].retweeted_status['full_text']\n",
    "    #     break\n",
    "    fav_counts = pd.pivot_table(data = pd.DataFrame(twitter_engagement),aggfunc = pd.Series.sum,index = 'hit_value',values = 'favorite_count')\n",
    "    rt_counts = pd.pivot_table(data = pd.DataFrame(twitter_engagement),aggfunc = pd.Series.sum,index = 'hit_value',values = 'retweet_count')\n",
    "    return fav_counts.merge(rt_counts,left_on = 'hit_value',right_on = 'hit_value').rename(columns = {'favorite_count':'twitter_favorite_count','retweet_count':'twitter_retweet_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_percentile(df: pd.DataFrame,col_name: str,threshold: int = 20,direction: str = 'top') -> pd.DataFrame:\n",
    "    df = df.sort_values(col_name,ascending = False)\n",
    "    index_to_fetch = int((threshold/100)*df.shape[0])\n",
    "    print(f'...{threshold} percent of {col_name}\\'s values are contained in {index_to_fetch} records')\n",
    "    return df.iloc[0:index_to_fetch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seedlist():\n",
    "    things_to_check = {'hashtags':['Elections2021'],'urls':['india-','-india','modi-','-modi']}\n",
    "    days = 7\n",
    "    date_from = datetime.now()\n",
    "    topic = 'youtube'\n",
    "    user_ids = []\n",
    "    list_ids = ['1151212983873396736']\n",
    "    for ht in things_to_check:\n",
    "        hits_df = check_hitcache(ht,things_to_check[ht],days,date_from,live = True,limit = False)\n",
    "        user_ids.extend(hits_df.node_id.unique().tolist())\n",
    "        users_df = get_top_percentile(get_all_users(user_ids),'followers_count',20)\n",
    "        users_df['method'] = 'hitcache'\n",
    "        members = []\n",
    "    for i in list_ids:\n",
    "        for page in tweepy.Cursor(api.list_members, list_id = i).items():\n",
    "            members.append(page)\n",
    "    temp_df = pd.DataFrame([x._json for x in members])\n",
    "    temp_df['method'] = 'lists'\n",
    "    users_df = users_df.append(temp_df)\n",
    "    to_write = {}\n",
    "    to_write['seeds'] = users_df\n",
    "    write_to_sheet(users_df,topic + ' Users Seedlist')\n",
    "    write_to_sheet(get_top_percentile(udf,'followers_count',20)[['id_str','name','screen_name','description','url']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphika_focus(feature_type: str ='hashtags',\n",
    "                   limit: int =100,\n",
    "                   map_id: str = '3108',\n",
    "                   segment: str = 'group',\n",
    "                   segment_id: int = 0,\n",
    "                   top: str = 'engagement_count'):\n",
    "    try:\n",
    "        u = f'https://api.graphika.com/focus/{map_id}/{segment}/{str(segment_id)}/{feature_type}?orderBy={top}&limit={limit}'\n",
    "        result = re.get(u, auth = (username,pswd))\n",
    "        result = pd.DataFrame(result.json()['results'])\n",
    "        result['segment_type'] = segment\n",
    "        return result\n",
    "    except:\n",
    "        print(f'No result for {segment} {segment_id} in {map_id} for {feature_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphika_chronotope(map_id: str = '3108',\n",
    "                        feature_type: str = 'hashtags',\n",
    "                        target: str = ''):\n",
    "    target = target.split('?')[0]\n",
    "    try:\n",
    "        q = f'https://api.graphika.com/chronotope/{map_id}/{feature_type}?itemId={target}&metadata=1'\n",
    "        r = re.get(q,auth = (username,pswd))\n",
    "        return pd.DataFrame(r.json())\n",
    "    except:\n",
    "        print(f'No result for {feature_type} {target} in {map_id}')\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_labels(labels):\n",
    "    labels_dict = {}\n",
    "    count = 0\n",
    "    for i in labels:\n",
    "        labels_dict[i] = [int((pd.Series(labels).value_counts().loc[i])/2),pd.Series(labels).value_counts().loc[i]]\n",
    "    start_pos = 0 \n",
    "    x = 0\n",
    "    for k in labels_dict:\n",
    "        labels_dict[k][0] += start_pos\n",
    "        labels_dict[k][1] += start_pos\n",
    "        start_pos = labels_dict[k][1]\n",
    "    count = 0\n",
    "    for i in labels:\n",
    "        if count == labels_dict[i][0]:\n",
    "            count += 1\n",
    "            continue\n",
    "        labels[count] = ''\n",
    "        count += 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def graphika_segments(map_id = '3108'):\n",
    "#     r = re.get(f'http://api.graphika.com/maps/{map_id}/groups', auth=(username, pswd))\n",
    "#     groups_df = pd.DataFrame(r.json())\n",
    "#     r = re.get(f'http://api.graphika.com/maps/{map_id}/clusters', auth=(username, pswd))\n",
    "#     clusters_df = pd.DataFrame(r.json())\n",
    "#     segments_df = clusters_df[['name','group_no','cluster_no']].rename(columns = {'name':'cluster_name'})\\\n",
    "#         .merge(groups_df[['group_no','name']].rename(columns = {'name':'group_name'}))\n",
    "#     segments_df = segments_df[['group_no','group_name','cluster_no','cluster_name']].sort_values('group_no')\n",
    "# #     display(segments_df)\n",
    "#     segments_df['map_id'] = map_id\n",
    "#     return segments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphika_segments(map_id: str) -> pd.DataFrame:\n",
    "    segments_df = pd.DataFrame(sb.Groups(map_id)).merge(pd.DataFrame(sb.Clusters(map_id)),left_on = 'group_no',right_on = 'group_no')\\\n",
    "    [['group_no','name_x','cluster_no','name_y']]\n",
    "    segments_df['map_id'] = map_id\n",
    "    segments_df.columns = [x[0] for x in segments_df.columns]\n",
    "    segments_df.columns = ['group_id','group_name','cluster_id','cluster_name','map_id']\n",
    "#     display(segments_df.columns)\n",
    "#     display(segments_df)\n",
    "    return segments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chronotope(hit_type: str, hit_value: str,days: int,map_id: str):\n",
    "    '''\n",
    "    Takes a hit type, value, day range, and map and returns a chronotope for it. Saves that to the pwd.\n",
    "    '''\n",
    "    print('...Fetching hits')\n",
    "    temp = check_hitcache(hit_type,hit_value,days,live = map_id, limit = False)\n",
    "    print('...Fetching map segments')\n",
    "    segments_df = pd.DataFrame()\n",
    "    temp = temp[temp.map_id.isin(temp.map_id.value_counts().index[:3].tolist())]\n",
    "    toplot = pd.pivot_table(temp, index = ['map_id','cluster_id','hit_time'],values = 'node_id',aggfunc = pd.Series.nunique).reset_index()\n",
    "    for m in temp.map_id.unique():\n",
    "#         t = graphika_segments(m)\n",
    "#         t['map_id'] = m\n",
    "        segments_df = segments_df.append(graphika_segments(m))\n",
    "    segments_df.cluster_id = segments_df.cluster_id.astype(str)\n",
    "    toplot = toplot.merge(segments_df[['map_id','cluster_id','group_name','cluster_name']],\\\n",
    "                          left_on = ['map_id','cluster_id'],right_on = ['map_id','cluster_id'])\n",
    "    toplot['segment'] =  toplot.group_name.values + ',' + toplot.cluster_name.values\n",
    "    print('...Printing chronotope')\n",
    "    toplot = toplot.sort_values('segment')\n",
    "    if map_id == True:\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.rcParams['figure.max_open_warning'] = 0\n",
    "        plt.rcParams[\"figure.figsize\"] = (65,50)\n",
    "        # alpha_val = float(x/temp.cluster_id.value_counts().values.max()) for x in \n",
    "    #     s = ax.scatter(toplot[toplot.map_id == m].hit_time, toplot[toplot.map_id == m].cluster_id.astype(int).sort_values().astype(str),\\\n",
    "    #                    c=toplot[toplot.map_id == m].cluster_id.astype(int),cmap = 'binary')\n",
    "        s = ax.scatter(toplot.hit_time, toplot.segment,\\\n",
    "                       c=toplot.cluster_id.astype(int),cmap = 'GnBu',s=300)\n",
    "\n",
    "        fig.suptitle('All Maps', fontsize=50)\n",
    "        plt.yticks(fontsize=25)\n",
    "        plt.xticks(rotation = 90,fontsize = 30)\n",
    "        plt.colorbar(s)\n",
    "\n",
    "        labels = [item.get_text().split(',')[0] for item in ax.get_yticklabels()]\n",
    "        labels = adjust_labels(labels)\n",
    "        ax.set_yticklabels(labels)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('nytimes_chronotope_all.jpeg')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        for m in toplot.map_id.value_counts().head(5).index.tolist():\n",
    "            fig, ax = plt.subplots()\n",
    "            plt.rcParams['figure.max_open_warning'] = 0\n",
    "            plt.rcParams[\"figure.figsize\"] = (100,50)\n",
    "            # alpha_val = float(x/temp.cluster_id.value_counts().values.max()) for x in \n",
    "        #     s = ax.scatter(toplot[toplot.map_id == m].hit_time, toplot[toplot.map_id == m].cluster_id.astype(int).sort_values().astype(str),\\\n",
    "        #                    c=toplot[toplot.map_id == m].cluster_id.astype(int),cmap = 'binary')\n",
    "            s = ax.scatter(toplot[toplot.map_id == m].hit_time, toplot[toplot.map_id == m].segment.sort_values(),\\\n",
    "                           c=toplot[toplot.map_id == m].cluster_id.astype(int),cmap = 'GnBu',s=300)\n",
    "\n",
    "            fig.suptitle('Map #'+str(m), fontsize=50)\n",
    "            plt.yticks(fontsize=25)\n",
    "            plt.xticks(rotation = 90,fontsize = 30)\n",
    "            plt.colorbar(s)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{hit_value}_chronotope_{m}.jpeg')\n",
    "            plt.show()\n",
    "    return toplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_influencers_base_report(filepath: str):\n",
    "    '''\n",
    "    Returns a merged table of retweets, mentioned, and replies tabs in a base report. Ignores follows.\n",
    "    '''\n",
    "    df = {}\n",
    "#     df['members'] = \n",
    "#     df['followers'] = report_df = pd.read_excel(filepath,'Follows')\n",
    "    df['retweets'] = report_df = pd.read_excel(filepath,'Retweets')\n",
    "    df['mentions'] = report_df = pd.read_excel(filepath,'Mentioned')\n",
    "    df['replies'] = report_df = pd.read_excel(filepath,'Replies')\n",
    "    res_df = pd.read_excel(filepath,'Follows').set_index('item')#[['screen name','service user id']]\n",
    "    left = '_followers'\n",
    "    res_df = res_df#.rename(columns = {'screen name':'item','service user id':'node_id'}).set_index('item')\n",
    "    for k in df:\n",
    "        right = '_' + k\n",
    "        temp = df[k][['item','Map Count'] + df[k].columns[df[k].columns.str.contains('\\(g\\)')].tolist()]\\\n",
    "                .set_index('item')\n",
    "#         print(left,right)\n",
    "#         display(res_df)\n",
    "#         display(temp)\n",
    "        res_df = res_df.merge(temp,left_index = True, right_index = True,suffixes = (left,right))\n",
    "        left = right\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bridges(base_report: str,heterophily_report: str,focus: int = 2):\n",
    "    '''\n",
    "    Use base and heterophily report to identify \"bridge\" accounts above a focus threshold of 2\n",
    "    '''\n",
    "    followers_df = pd.read_excel(base_report,'Follows').set_index('item')\n",
    "    heterophily = pd.read_excel(heterophily_report).set_index('Unnamed: 0')\n",
    "    bridge_df = []\n",
    "    least_phily = []\n",
    "    for row in heterophily.iterrows():\n",
    "        min_phily = row[1].min()\n",
    "        for k in row[1].keys():\n",
    "            if row[1][k] == min_phily:\n",
    "                least_phily.append({'group A':row[0],'group B':k})\n",
    "                break\n",
    "    least_phily = pd.DataFrame(least_phily)\n",
    "    for f in followers_df.iterrows():\n",
    "        for p in least_phily.iterrows():\n",
    "            if f[1][p[1]['group A']] >= focus and f[1][p[1]['group B']] >= focus:\n",
    "                bridge_df.append({'user':f[0],'from':p[1]['group A'],'to':p[1]['group B']})\n",
    "    return pd.DataFrame(bridge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tags(sheet: str = 'current'):\n",
    "#     temp = pd.read_excel('./clustermap_tags.xlsx').set_index(['map_id','clu_id'])\n",
    "#     temp = pd.read_excel('./all_tags.xlsx').set_index(['map_id','cluster_id'])\n",
    "    temp = pd.read_excel('/Users/avneeshchandra/Analyst_tools/avneesh/all_tags.xlsx',sheet).set_index(['map_id','cluster_id'])\n",
    "#     temp.index = temp.index.astype(str)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = load_tags()\n",
    "map_id_dict = pd.DataFrame(sb.Maps())[['id','name']].set_index('id').to_dict()['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_lookup(rcvd_series):\n",
    "    \"\"\"\n",
    "    This function returns a Series of tags\n",
    "    Inputs:\n",
    "        rcvd_series: An input of cluster IDs\n",
    "    Outputs:\n",
    "        load_tags().loc[map_id,cluster_id]['TAG']: Tag name\n",
    "    \"\"\"\n",
    "    global all_tags\n",
    "    map_id = int(rcvd_series['map_id'])\n",
    "    cluster_id = int(rcvd_series['cluster_no'])\n",
    "#     unique_id = str(map_id) + '.' + str(cluster_id)\n",
    "#     display(rcvd_series)\n",
    "    try:\n",
    "        return all_tags.loc[map_id,cluster_id]['tag']\n",
    "    except:\n",
    "        return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tags(df: pd.DataFrame):\n",
    "#     df.map_id = df.map_id.astype(str)\n",
    "#     df.cluster_id = df.cluster_id.astype(str)\n",
    "    df = df.rename(columns = {'cluster_id':'cluster_no'})\n",
    "    tag_values = df.apply(tag_lookup,axis = 1,result_type = 'reduce')\n",
    "    df.insert(loc = df.shape[1]-1,column = 'tag', value = tag_values)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
