{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run db_connect.ipynb\n",
    "# %run misc_functions.ipynb\n",
    "# api_creds = graphika_api_login()\n",
    "# username = api_creds[\"username\"]\n",
    "# pswd = api_creds[\"password\"]\n",
    "# %run connect_to_api.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tweets = pd.DataFrame()\n",
    "map_hashtags = pd.DataFrame()\n",
    "map_urls = pd.DataFrame()\n",
    "map_retweets = pd.DataFrame()\n",
    "map_feature_data = pd.DataFrame()\n",
    "map_id = None\n",
    "map_features = {'hashtags':pd.DataFrame(),'urls':pd.DataFrame(),'retweets':pd.DataFrame()}\n",
    "debug_opt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_data(map_id, grouped = True):\n",
    "    \n",
    "    \n",
    "    #This data should also be saved\n",
    "    url = \"https://api.graphika.com/clustermaps/{}/nodes\".format(map_id)\n",
    "    r = requests.get(url,auth=(username, pswd))\n",
    "    node_data = r.json()\n",
    "\n",
    "    url = \"https://api.graphika.com/clustermaps/{}/clusters\".format(map_id)\n",
    "    r = requests.get(url,auth=(username, pswd))\n",
    "    cluster_data = r.json()\n",
    "\n",
    "\n",
    "    url = \"https://api.graphika.com/clustermaps/{}/groups\".format(map_id)\n",
    "    r = requests.get(url,auth=(username, pswd))\n",
    "    group_data = r.json()\n",
    "\n",
    "\n",
    "    df_nodes = pd.DataFrame({\"screen_name\":[n[\"screenname\"] for n in node_data],\\\n",
    "                            \"node_id\":[n[\"service_user_id\"] for n in node_data],\\\n",
    "                             \"cluster_id\":[n[\"attentive_cluster_id\"] for n in node_data]})\n",
    "    df_clusters = pd.DataFrame({\"cluster_id\":[n[\"id\"] for n in cluster_data[\"clusters\"]], \\\n",
    "                                \"cluster_name\": [n[\"name\"] for n in cluster_data[\"clusters\"]],\\\n",
    "                                \"group_id\": [n[\"group\"] for n in cluster_data[\"clusters\"]]})\n",
    "    df_clusters[\"cluster_id\"] = df_clusters[\"cluster_id\"].astype(\"int\")\n",
    "    df_group = pd.DataFrame({\"group_id\":[n for n in group_data], \\\n",
    "                            \"group_name\":[v[\"name\"] for v in group_data.values()]})\n",
    "\n",
    "    df0 = pd.merge(df_nodes, df_clusters,on = \"cluster_id\")\n",
    "    \n",
    "    if grouped:\n",
    "    \n",
    "        df0 = pd.merge(df0,df_group, on = \"group_id\")\n",
    "    \n",
    "        df0 = df0[[\"screen_name\",\"node_id\",\"cluster_name\",\"group_name\"]]\n",
    "        \n",
    "        \n",
    "    df0[\"node_id\"] = df0[\"node_id\"].astype(\"str\")\n",
    "\n",
    "    return df0\n",
    "\n",
    "\n",
    "def get_screen_names(map_id):\n",
    "    url = \"https://api.graphika.com/clustermaps/%s/nodes\" %map_id\n",
    "    r = requests.get(url,auth=(username, pswd))\n",
    "    node_hash = r.json()\n",
    "    screen_names = [x['screenname'] for x in node_hash]\n",
    "    service_ids = [x[\"service_user_id\"] for x in node_hash]\n",
    "    d = {'screen_name':screen_names,'id':service_ids}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_data(feature, map_id, use_map_dates = True, date_from = False, date_to = False, case = \"default\"):\n",
    "    '''This searches within a single map with the option of using the map's creation dates as a param'''\n",
    "    global debug_opt\n",
    "    if use_map_dates:\n",
    "        date_from, date_to = get_map_dates (map_id)\n",
    "    limit = ''\n",
    "    if debug_opt:\n",
    "        limit = ' LIMIT 1000'\n",
    "    \n",
    "    if case == \"default\":\n",
    "        query = \"SELECT * FROM \"\\\n",
    "        \"(SELECT message_id, hits_twitter_{}.node_id, hit_time, hit_value as hit, map_nodes.map_id FROM hits_twitter_{} \\\n",
    "        join map_nodes on map_nodes.node_id = hits_twitter_{}.node_id) s \\\n",
    "        where s.map_id = {} \\\n",
    "        and s.hit_time BETWEEN '{}'::TIMESTAMP AND '{}'::TIMESTAMP{};;\".format(feature, feature, feature, map_id, date_from, date_to,limit)\n",
    "    \n",
    "    if case == \"standardize\":\n",
    "        query = \"SELECT * FROM \"\\\n",
    "        \"(SELECT message_id, hits_twitter_{}.node_id, hit_time, lower(hit_value) as hit, map_nodes.map_id FROM hits_twitter_{} \\\n",
    "        join map_nodes on map_nodes.node_id = hits_twitter_{}.node_id) s \\\n",
    "        where s.map_id = {} \\\n",
    "        and s.hit_time BETWEEN '{}'::TIMESTAMP AND '{}'::TIMESTAMP{};;\".format(feature, feature, feature, map_id, date_from, date_to,limit)\n",
    "\n",
    "    r = cur.execute(query)\n",
    "    print('...Querying database')\n",
    "    \n",
    "    hits = cur.fetchall()\n",
    "    print('...Morphing dataframe')\n",
    "    hits_df = pd.DataFrame(hits, columns=[\"message_id\",\"node_id\", \"time\", \"hit_value\", \"map_id\"])\n",
    "    hits_df[\"node_id\"] = hits_df[\"node_id\"].astype(\"str\")\n",
    "    hits_df[\"message_id\"] = hits_df[\"message_id\"].astype(\"str\")\n",
    "    hits_df[\"hit_type\"] = feature\n",
    "\n",
    "    return hits_df\n",
    "\n",
    "\n",
    "def get_map_dates(map_id):\n",
    "    url = \"https://api.graphika.com/clustermaps/{}\".format(map_id)\n",
    "    r = requests.get(url,auth=(username, pswd))\n",
    "    map_data = r.json()\n",
    "    map_dates = map_data[\"date_range\"]\n",
    "    later_date = datetime.utcfromtimestamp(map_dates[1]).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    earlier_date = datetime.utcfromtimestamp(map_dates[0]).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return earlier_date,later_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_hits(feature,map_id):\n",
    "    print('...Getting map nodes')\n",
    "    map_nodes = get_node_data(map_id)\n",
    "    print('...Getting hits')\n",
    "    hits = get_hits_data(feature,map_id)\n",
    "    print('...Merging nodes with hits')\n",
    "    map_hits = pd.merge(map_nodes,hits)\n",
    "    print('...Done!')\n",
    "    return map_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_map_activity_report(debug = False):\n",
    "    global debug_opt\n",
    "    debug_opt = debug\n",
    "    global map_id\n",
    "    global map_tweets\n",
    "    \n",
    "    input_map_id = input(\">> Enter map id: \")\n",
    "    if input_map_id != map_id:\n",
    "        print('...Fetching map data')\n",
    "        map_id = input_map_id\n",
    "        map_tweets = get_map_hits(\"tweets\",map_id)\n",
    "    else:\n",
    "        print('...Map data found!')\n",
    "    result = sort_by_count(map_tweets)\n",
    "    display(result)\n",
    "    if input('>> Do you want to save this result to a CSV? (y/n) \\n') == 'y':\n",
    "        print_csv(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_count(df):\n",
    "    count_by = input(\"count by group, cluster, or account: \")\n",
    "    countby_choice = {'group':'group_name','cluster':'cluster_name','account':'screen_name'}\n",
    "    try:\n",
    "        activity_counts = map_tweets.groupby(countby_choice[count_by])[\"message_id\"].count()\n",
    "        activity_counts.name = count_by + \"_tweet_count\"\n",
    "    except:\n",
    "        print (\"**Not a valid level to count tweets by**\")\n",
    "        return None\n",
    "\n",
    "    activity_counts.sort_values(ascending = False, inplace = True)\n",
    "    return pd.DataFrame(activity_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_activity_report(debug = False):\n",
    "    \n",
    "    global debug_opt\n",
    "    debug_opt = debug\n",
    "    global map_id\n",
    "    global map_features\n",
    "    global map_feature_data\n",
    "        \n",
    "        \n",
    "    input_map_id = input(\">> Enter map id: \")\n",
    "    feature_type = input(\">> Search for hashtags, urls, or retweets: \")\n",
    "    case_sensitive = input(\">> Is this search case sensitive? (y/n) \\n\")\n",
    "    feature_value = (input (\">> Comma separate search parameters, or hit enter for all: \"))\n",
    "    \n",
    "    if case_sensitive == 'n':\n",
    "        case_sensitive = False\n",
    "    else:\n",
    "        case_sensitive = True\n",
    "    feature_value = feature_value.split(\",\")\n",
    "\n",
    "    #Case densitivity option should be available\n",
    "        \n",
    "    if input_map_id != map_id:\n",
    "        print('...Fetching map data')\n",
    "        map_id = input_map_id\n",
    "        map_features = {'hashtags':pd.DataFrame(),'urls':pd.DataFrame(),'retweets':pd.DataFrame()}\n",
    "    if map_features[feature_type].empty:\n",
    "        print('...Fetching {} data'.format(feature_type))\n",
    "        map_features[feature_type] = get_map_hits(feature_type,map_id)\n",
    "    \n",
    "    map_feature_data = map_features[feature_type]\n",
    "        \n",
    "    if feature_value:\n",
    "        feature_results = pd.DataFrame()\n",
    "        for searchterm in feature_value:\n",
    "            searchterm = searchterm.strip()\n",
    "            print('...Searching {} for <{}>'.format(feature_type,searchterm))\n",
    "            feature_results = feature_results.append(map_feature_data[map_feature_data.hit_value.str.contains(searchterm,case=case_sensitive)])\n",
    "\n",
    "        if not feature_results.empty:\n",
    "        \n",
    "            user_counts = pd.DataFrame(feature_results.groupby(\"screen_name\").count()[\"hit_value\"])\n",
    "            user_counts.columns = [\"number_of_tweets\"]\n",
    "\n",
    "            user_counts.sort_values(by = \"number_of_tweets\", inplace= True, ascending = False)\n",
    "            \n",
    "            if input('>> Do you want to save the search results to a CSV? (y/n) \\n') == 'y':\n",
    "                print_csv(feature_results)\n",
    "            display(user_counts)\n",
    "            if input('>> Do you want to save the above table to a CSV? (y/n) \\n') == 'y':\n",
    "                print_csv(user_counts)\n",
    "            \n",
    "        else:\n",
    "            print (\"**No results for search term entered**\")\n",
    "    \n",
    "    else:\n",
    "        if input('>> Do you want to save the search results to a CSV? (y/n) \\n') == 'y':\n",
    "            print_csv(feature_results)\n",
    "        return feature_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
