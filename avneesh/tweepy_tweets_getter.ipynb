{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import tweepy\n",
    "import csv\n",
    "from tweepy import Cursor\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "                                                                     \n",
    "import pprint as pp\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'HWAATgWiwSpVWYTfM2M8zoKyh'\n",
    "consumer_secret = 'Qtk2aJ0kaOj3B2OWdWa0ca0fXYCrq1AJiYTRlXHHlFPOFoCaG2'\n",
    "access_token = '1008796219622256641-h021XV80GH2sy7t2wocOOwzymL0ONw'\n",
    "access_token_secret = '7bgHU7uUEdvNW2KpYoCiKOOR15B6ihvmkHVzVXQUinboQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# from requests_oauthlib import OAuth1\n",
    "# \n",
    "# \n",
    "# base_twitter_url = \"https://api.twitter.com/1.1/\"\n",
    "# \n",
    "# auth = OAuth1(client_key, client_secret, token, token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets (term, map_id = False, grouped = True, limit = 1):\n",
    "    \"\"\"\n",
    "    This function uses Tweepy to fetch Twitter data\n",
    "    Inputs:\n",
    "        term: Search term\n",
    "        map_id: Which map to merge data with\n",
    "        grouped: Whether to merge with group data or not\n",
    "        limit: How many 100s of search results to fetch\n",
    "    Outputs:\n",
    "        df: Dataframe of tweets returned from search\n",
    "    \"\"\"\n",
    "    alltweets = []\n",
    "    new_tweets = api.search(term, count = 100,tweet_mode='extended')\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    counter = 0\n",
    "\n",
    "#     while len(new_tweets) > 0:\n",
    "    if limit == False:\n",
    "        print('...Fetching all tweets')\n",
    "        limit = 99999\n",
    "    else:\n",
    "        print('..Fetching {}00 tweets'.format(limit))\n",
    "        \n",
    "    for i in range(limit):\n",
    "        #break out if no more tweets are found\n",
    "        if len(new_tweets) == 0:\n",
    "            break\n",
    "#         print (\"...Getting tweets before %s\" %(oldest))\n",
    "\n",
    "        #all subsequent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.search(term, count = 100,max_id=oldest,tweet_mode='extended')\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        return parse_tweepy_results(alltweets)\n",
    "#         print (\"...%s tweets downloaded so far\"%(len(alltweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweepy_results(alltweets, map_id = False, grouped = True):\n",
    "    tweets_features = []\n",
    "\n",
    "    for tweet in alltweets:\n",
    "        tweet_dict = {}\n",
    "        if str(tweet.full_text)[:3] == \"RT \":\n",
    "            try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "                retweet = True\n",
    "                retweet_screen_name = tweet.retweeted_status.user.screen_name\n",
    "                retweet_tweet_id = tweet.retweeted_status.id_str\n",
    "                urls = []\n",
    "                for u in tweet.retweeted_status.entities[\"urls\"]:\n",
    "                    urls.append(u[\"expanded_url\"])\n",
    "\n",
    "                hashtags = []\n",
    "                for h in tweet.retweeted_status.entities[\"hashtags\"]:\n",
    "                    hashtags.append(h['text'])\n",
    "\n",
    "                mentions = []    \n",
    "                for m in tweet.retweeted_status.entities[\"user_mentions\"]:\n",
    "                    mentions.append(m['screen_name'])\n",
    "\n",
    "            except AttributeError:\n",
    "                retweet = True\n",
    "                text = tweet.full_text.encode(\"utf-8\")\n",
    "\n",
    "        else:\n",
    "            text = tweet.full_text.encode(\"utf-8\") #indented this here\n",
    "            retweet = False\n",
    "            retweet_screen_name = None\n",
    "            retweet_tweet_id = None\n",
    "\n",
    "        urls = []\n",
    "        for u in tweet.entities[\"urls\"]:\n",
    "            urls.append(u[\"expanded_url\"])\n",
    "\n",
    "        hashtags = []\n",
    "        for h in tweet.entities[\"hashtags\"]:\n",
    "            hashtags.append(h['text'])\n",
    "\n",
    "        mentions = []    \n",
    "        for m in tweet.entities[\"user_mentions\"]:\n",
    "            mentions.append(m['screen_name'])\n",
    "\n",
    "        tweet_dict[\"retweet_screen_name\"] = retweet_screen_name\n",
    "        tweet_dict[\"retweet_tweet_id\"] = retweet_tweet_id \n",
    "        tweet_dict[\"user_id\"] = tweet.user.id\n",
    "        tweet_dict[\"retweet\"] = retweet\n",
    "        tweet_dict[\"text\"] = text\n",
    "        tweet_dict[\"author\"] = tweet.author.name\n",
    "        tweet_dict[\"screen_name\"] = tweet.author.screen_name\n",
    "        tweet_dict[\"tweet_id\"] = tweet.id_str\n",
    "        tweet_dict[\"time\"] = tweet.created_at\n",
    "        tweet_dict[\"mentions\"] = mentions\n",
    "        tweet_dict[\"hashtags\"] = hashtags\n",
    "        tweet_dict[\"urls\"] = urls\n",
    "\n",
    "        tweets_features.append(tweet_dict)\n",
    "\n",
    "        #also need author id so can recreated url, might also be an easier way\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "    # outtweets = [[tweet.author.name, tweet.id_str, tweet.created_at, tweet.full_text.encode(\"utf-8\")] for tweet in alltweets]\n",
    "\n",
    "    df = pd.DataFrame(tweets_features)\n",
    "    \n",
    "    if map_id:\n",
    "        nodes = get_node_data(map_id,grouped)\n",
    "        df = pd.merge(nodes,df)\n",
    "        \n",
    "    print('...Done!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_tweets(tweet_id_list):\n",
    "    print('...Getting {} tweets'.format(len(tweet_id_list)))\n",
    "#     tweet_id_list = [1246082527544180736,1246086302178537473]\n",
    "    result_tweets = []\n",
    "    for i in range(0,len(tweet_id_list),100):\n",
    "        try:\n",
    "            result_tweets.extend(api.statuses_lookup(tweet_id_list[i:i+100],tweet_mode = 'extended'))\n",
    "        except:\n",
    "            continue\n",
    "    return parse_tweepy_results(result_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
