{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run misc_functions.ipynb\n",
    "from datetime import datetime\n",
    "# %run tweepy_tweets_getter.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tweets = pd.DataFrame()\n",
    "map_hashtags = pd.DataFrame()\n",
    "map_urls = pd.DataFrame()\n",
    "map_retweets = pd.DataFrame()\n",
    "map_feature_data = pd.DataFrame()\n",
    "map_id = None\n",
    "map_features = {'hashtags':pd.DataFrame(),'urls':pd.DataFrame(),'retweets':pd.DataFrame()}\n",
    "debug_opt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns a dataframe with all nodes of a map after fetching their segment data\n",
    "\n",
    "def get_node_data(map_id, grouped = True):\n",
    "    \"\"\"\n",
    "    This function returns all nodes of a given map with their segment data\n",
    "    Inputs:\n",
    "        map_id: Map ID for which to fetch node data\n",
    "        grouped: Whether or not to merge returned df on group data\n",
    "    Outputs:\n",
    "        df0: Dataframe of all nodes in a map with segment data included\n",
    "    \"\"\"\n",
    "    \n",
    "    #This data should also be saved\n",
    "    url = \"https://api.graphika.com/clustermaps/{}/nodes\".format(map_id)\n",
    "    r = requests.get(url,auth=(username, pswd))\n",
    "    node_data = r.json()\n",
    "\n",
    "    url = \"https://api.graphika.com/clustermaps/{}/clusters\".format(map_id)\n",
    "    r = requests.get(url,auth=(username, pswd))\n",
    "    cluster_data = r.json()\n",
    "\n",
    "\n",
    "    url = \"https://api.graphika.com/clustermaps/{}/groups\".format(map_id)\n",
    "    r = requests.get(url,auth=(username, pswd))\n",
    "    group_data = r.json()\n",
    "\n",
    "\n",
    "    df_nodes = pd.DataFrame({\"screen_name\":[n[\"screenname\"] for n in node_data],\\\n",
    "                            \"node_id\":[n[\"service_user_id\"] for n in node_data],\\\n",
    "                             \"cluster_id\":[n[\"attentive_cluster_id\"] for n in node_data]})\n",
    "    df_clusters = pd.DataFrame({\"cluster_id\":[n[\"id\"] for n in cluster_data[\"clusters\"]], \\\n",
    "                                \"cluster_name\": [n[\"name\"] for n in cluster_data[\"clusters\"]],\\\n",
    "                                \"group_id\": [n[\"group\"] for n in cluster_data[\"clusters\"]]})\n",
    "    df_clusters[\"cluster_id\"] = df_clusters[\"cluster_id\"].astype(\"int\")\n",
    "    df_group = pd.DataFrame({\"group_id\":[n for n in group_data], \\\n",
    "                            \"group_name\":[v[\"name\"] for v in group_data.values()]})\n",
    "\n",
    "    df0 = pd.merge(df_nodes, df_clusters,on = \"cluster_id\")\n",
    "    \n",
    "    if grouped:\n",
    "    \n",
    "        df0 = pd.merge(df0,df_group, on = \"group_id\")\n",
    "    \n",
    "        df0 = df0[[\"screen_name\",\"node_id\",\"cluster_name\",\"group_name\",\"cluster_id\"]]\n",
    "        \n",
    "        \n",
    "    df0[\"node_id\"] = df0[\"node_id\"].astype(\"str\")\n",
    "\n",
    "    return df0\n",
    "\n",
    "\n",
    "def get_screen_names(map_id):\n",
    "    \"\"\"\n",
    "    This function returns all names of nodes in a given map\n",
    "    Inputs:\n",
    "        map_id: Map ID for which to fetch node data\n",
    "    Outputs:\n",
    "        df0: Dataframe of all nodes in a map with node names included\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://api.graphika.com/clustermaps/%s/nodes\" %map_id\n",
    "    r = requests.get(url,auth=(username, pswd))\n",
    "    node_hash = r.json()\n",
    "    screen_names = [x['screenname'] for x in node_hash]\n",
    "    service_ids = [x[\"service_user_id\"] for x in node_hash]\n",
    "    d = {'screen_name':screen_names,'id':service_ids}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_data(feature, map_id, use_map_dates = True, date_from = False, date_to = False, case = \"default\"):\n",
    "    \"\"\"\n",
    "    This function executes a query on the hitcache and returns the result\n",
    "    Inputs:\n",
    "        feature: Which feature to look at, ie. hashtags, URLs, mentions\n",
    "        map_id: Map ID for which to fetch hitcache data\n",
    "        use_map_dates: Whether or not to use the first and last dates of the data collected in the map\n",
    "        date_from: Manually entered beginning date\n",
    "        date_to: Manually entered ending date\n",
    "        case: Whether or not the search is case-sensitive\n",
    "    Outputs:\n",
    "        hits_df: Resulting dataframe from the query after some column name and type alterations\n",
    "    \"\"\"\n",
    "    global debug_opt\n",
    "    if use_map_dates:\n",
    "        date_from, date_to = get_map_dates(map_id)\n",
    "    limit = ''\n",
    "    if debug_opt:\n",
    "        limit = ' LIMIT 10000'\n",
    "    \n",
    "    if case == \"default\":\n",
    "        query = \"SELECT * FROM \"\\\n",
    "        \"(SELECT message_id, hits_twitter_{}.node_id, hit_time, hit_value as hit, map_nodes.map_id FROM hits_twitter_{} \\\n",
    "        join map_nodes on map_nodes.node_id = hits_twitter_{}.node_id) s \\\n",
    "        where s.map_id = {} \\\n",
    "        and s.hit_time BETWEEN '{}'::TIMESTAMP AND '{}'::TIMESTAMP{};;\".format(feature, feature, feature, map_id, date_from, date_to,limit)\n",
    "    \n",
    "    if case == \"standardize\":\n",
    "        query = \"SELECT * FROM \"\\\n",
    "        \"(SELECT message_id, hits_twitter_{}.node_id, hit_time, lower(hit_value) as hit, map_nodes.map_id FROM hits_twitter_{} \\\n",
    "        join map_nodes on map_nodes.node_id = hits_twitter_{}.node_id) s \\\n",
    "        where s.map_id = {} \\\n",
    "        and s.hit_time BETWEEN '{}'::TIMESTAMP AND '{}'::TIMESTAMP{};;\".format(feature, feature, feature, map_id, date_from, date_to,limit)\n",
    "\n",
    "    r = cur.execute(query)\n",
    "    print('...Querying database')\n",
    "    \n",
    "    hits = cur.fetchall()\n",
    "    print('...Morphing dataframe')\n",
    "    hits_df = pd.DataFrame(hits, columns=[\"message_id\",\"node_id\", \"time\", \"hit_value\", \"map_id\"])\n",
    "    hits_df[\"node_id\"] = hits_df[\"node_id\"].astype(\"str\")\n",
    "    hits_df[\"message_id\"] = hits_df[\"message_id\"].astype(\"str\")\n",
    "    hits_df[\"hit_type\"] = feature\n",
    "\n",
    "    return hits_df\n",
    "\n",
    "\n",
    "def get_map_dates(map_id):\n",
    "    \"\"\"\n",
    "    This function returns the first and last dates of the data collected in a map\n",
    "    Inputs:\n",
    "        map_id: Map for which to fetch dates\n",
    "    Outputs:\n",
    "        earlier_date: the first date of data collected in the map\n",
    "        later_date: the last date of data collected in the map\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://api.graphika.com/clustermaps/{}\".format(map_id)\n",
    "    r = requests.get(url,auth=(username, pswd))\n",
    "    map_data = r.json()\n",
    "    map_dates = map_data[\"date_range\"]\n",
    "    later_date = datetime.utcfromtimestamp(map_dates[1]).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    earlier_date = datetime.utcfromtimestamp(map_dates[0]).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return earlier_date,later_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_hits(feature,map_id):\n",
    "    \"\"\"\n",
    "    This function gets first the node data from a map, then gets its associated hits, and returns the merged result\n",
    "    Inputs:\n",
    "        feature: Which feature to look at, ie. hashtags, URLs, mentions\n",
    "        map_id: Map ID for which to fetch hitcache data\n",
    "    Outputs:\n",
    "        map_hits: Resulting merged dataframe with both a map's node data and its hits\n",
    "    \"\"\"\n",
    "    \n",
    "    print('...Getting map nodes')\n",
    "    map_nodes = get_node_data(map_id)\n",
    "    print('...Getting hits')\n",
    "    hits = get_hits_data(feature,map_id)\n",
    "    print('...Merging nodes with hits')\n",
    "    map_hits = pd.merge(map_nodes,hits)\n",
    "    print('...Merging nodes with tags')\n",
    "    map_hits = merge_tags(map_hits)\n",
    "    print('...Done!')\n",
    "    return map_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_map_activity_report(debug = False):\n",
    "    \"\"\"\n",
    "    This function runs the entirety of the Map Activity Report (focused on segment data)\n",
    "    Inputs:\n",
    "        debug: A Boolean to limit the amount of data pulled and processed for debugging purposes\n",
    "    Outputs:\n",
    "        result: The resulting Map Activity dataframe with both nodes and hits data\n",
    "    \"\"\"\n",
    "    global debug_opt\n",
    "    debug_opt = debug\n",
    "    global map_id\n",
    "    global map_tweets\n",
    "    \n",
    "    input_map_id = input(\">> Enter map id: \")\n",
    "    if input_map_id != map_id:\n",
    "        print('...Fetching map data')\n",
    "        map_id = input_map_id\n",
    "        map_tweets = get_map_hits(\"tweets\",map_id)\n",
    "    else:\n",
    "        print('...Map data found!')\n",
    "    result = sort_by_count(map_tweets)\n",
    "    display(result)\n",
    "    if input('>> Do you want to save this result to a CSV? (y/n) \\n') == 'y':\n",
    "        print_csv(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_count(df):\n",
    "    \"\"\"\n",
    "    This function sorts the input dataframe by the user's preferred segment (group, cluster, or nodes)\n",
    "    Inputs:\n",
    "        df: The input dataframe of nodes and hits data\n",
    "    Outputs:\n",
    "        activity_counts: Resulting sorted dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    count_by = input(\">> Please enter which to aggregate the data by – tag, group, cluster, or account: \")\n",
    "    countby_choice = {'group':'group_name','cluster':'cluster_name','account':'screen_name','tag':'tag'}\n",
    "    try:\n",
    "        activity_counts = map_tweets.groupby(countby_choice[count_by])[\"message_id\"].count()\n",
    "        activity_counts.name = count_by + \"_tweet_count\"\n",
    "    except:\n",
    "        print (\"**Not a valid level to count tweets by**\")\n",
    "        return None\n",
    "\n",
    "    activity_counts.sort_values(ascending = False, inplace = True)\n",
    "    return pd.DataFrame(activity_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_activity_report(debug = False):\n",
    "    \"\"\"\n",
    "    This function runs the entirety of the Feature Activity Report (focused on hits data)\n",
    "    Inputs:\n",
    "        debug: A Boolean to limit the amount of data pulled and processed for debugging purposes\n",
    "    Outputs:\n",
    "        result: The resulting Map Activity dataframe with both nodes and hits data\n",
    "    \"\"\"\n",
    "    global debug_opt\n",
    "    debug_opt = debug\n",
    "    global map_id\n",
    "    global map_features\n",
    "    global map_feature_data\n",
    "        \n",
    "        \n",
    "    input_map_id = input(\">> Enter map id: \")\n",
    "    feature_type = input(\">> Search for hashtags, urls, or retweets: \")\n",
    "    case_sensitive = input(\">> Is this search case sensitive? (y/n) \\n\")\n",
    "    feature_value = (input (\">> Comma separate search parameters, or hit enter for all: \"))\n",
    "    \n",
    "    if case_sensitive == 'n':\n",
    "        case_sensitive = False\n",
    "    else:\n",
    "        case_sensitive = True\n",
    "    feature_value = feature_value.split(\",\")\n",
    "\n",
    "    #Case densitivity option should be available\n",
    "        \n",
    "    if input_map_id != map_id:\n",
    "        print('...Fetching map data')\n",
    "        map_id = input_map_id\n",
    "        map_features = {'hashtags':pd.DataFrame(),'urls':pd.DataFrame(),'retweets':pd.DataFrame()}\n",
    "    if map_features[feature_type].empty:\n",
    "        print('...Fetching {} data'.format(feature_type))\n",
    "        map_features[feature_type] = get_map_hits(feature_type,map_id)\n",
    "    \n",
    "    map_feature_data = map_features[feature_type]\n",
    "        \n",
    "    if feature_value:\n",
    "        feature_results = pd.DataFrame()\n",
    "        for searchterm in feature_value:\n",
    "            searchterm = searchterm.strip()\n",
    "            print('...Searching {} for <{}>'.format(feature_type,searchterm))\n",
    "            feature_results = feature_results.append(map_feature_data[map_feature_data.hit_value.str.contains(searchterm,case=case_sensitive)])\n",
    "\n",
    "        if not feature_results.empty:\n",
    "        \n",
    "            user_counts = pd.DataFrame(feature_results.groupby(\"screen_name\").count()[\"hit_value\"])\n",
    "            user_counts.columns = [\"number_of_tweets\"]\n",
    "\n",
    "            user_counts.sort_values(by = \"number_of_tweets\", inplace= True, ascending = False)\n",
    "            \n",
    "            display(feature_results)\n",
    "            if input('>> Do you want to save the search results to a CSV? (y/n) \\n') == 'y':\n",
    "                print_csv(feature_results)\n",
    "            display(user_counts)\n",
    "            if input('>> Do you want to save the above table to a CSV? (y/n) \\n') == 'y':\n",
    "                print_csv(user_counts)\n",
    "            \n",
    "        else:\n",
    "            print (\"**No results for search term entered**\")\n",
    "    \n",
    "    else:\n",
    "        if input('>> Do you want to save the search results to a CSV? (y/n) \\n') == 'y':\n",
    "            print_csv(feature_results)\n",
    "        return feature_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_full_report(debug = False):\n",
    "# #     feature,map_id, q, search_type, date_from = \"map_from\", date_to = \"map_to\"\n",
    "#     global debug_opt\n",
    "#     debug_opt = debug\n",
    "#     global map_id\n",
    "#     global map_features\n",
    "#     global map_feature_data\n",
    "#     input_map_id = input(\">> Enter map id: \")\n",
    "#     feature_type = input(\">> Search for hashtags, urls, or retweets: \")\n",
    "#     case_sensitive = input(\">> Is this search case sensitive? (y/n) \\n\")\n",
    "# #     feature_value = (input (\">> Comma separate search parameters, or hit enter for all: \"))\n",
    "    \n",
    "#     if case_sensitive == 'n':\n",
    "#         case_sensitive = False\n",
    "#     else:\n",
    "#         case_sensitive = True\n",
    "# #     feature_value = feature_value.split(\",\")\n",
    "\n",
    "#     #Case densitivity option should be available\n",
    "        \n",
    "#     if input_map_id != map_id:\n",
    "#         print('...Fetching map data')\n",
    "#         map_id = input_map_id\n",
    "#         map_features = {'hashtags':pd.DataFrame(),'urls':pd.DataFrame(),'retweets':pd.DataFrame()}\n",
    "#     if map_features[feature_type].empty:\n",
    "#         print('...Fetching {} data'.format(feature_type))\n",
    "#         map_features[feature_type] = get_map_hits(feature_type,map_id)\n",
    "    \n",
    "#     map_feature_data = map_features[feature_type]\n",
    "    \n",
    "# #     hits_df = get_hits_data(feature, map_id, date_from, date_to)\n",
    "#     hits_df = maps_feature_data\n",
    "\n",
    "# #     if search_type == \"contains\":\n",
    "# #         results_df = hits_df[hits_df.hit_value.str.lower().apply(lambda x: any(term in x for term in q))]\n",
    "\n",
    "# #     if search_type == \"exact\":\n",
    "# #         results_df = hits_df[hits_df.hit_value.str.lower().isin(q)]\n",
    "\n",
    "# #     nodes = get_node_data(map_id)\n",
    "# #     nodes.node_id = nodes.node_id.astype(str)    \n",
    "\n",
    "# #     df = pd.merge(results_df,nodes,left_on=\"node_id\",right_on=\"node_id\")\n",
    "\n",
    "#     count_by = input(\">> Please enter which to aggregate the data by – tag, group, cluster, or account: \")\n",
    "#     countby_choice = {'group':'group_name','cluster':'cluster_name','account':'screen_name','tag':'tag'}\n",
    "    \n",
    "#     feature_count_tweets = df.pivot_table(index=\"hit_value\",columns=countby_choice,aggfunc=\"count\",values=\"message_id\").fillna(0)\n",
    "#     feature_count_tweets[\"total\"] = feature_count_tweets.sum(axis=1)\n",
    "#     feature_count_tweets.sort_values(by=\"total\", ascending=False)\n",
    "\n",
    "#     feature_count_part = df.pivot_table(index=\"hit_value\",columns=countby_choice,aggfunc=pd.Series.nunique,values=\"node_id\").fillna(0)\n",
    "#     feature_count_part[\"total\"] = feature_count_tweets.sum(axis=1)\n",
    "#     feature_count_part.sort_values(by=\"total\", ascending=False)\n",
    "\n",
    "#     df.drop_duplicates([\"message_id\"], inplace=True)\n",
    "\n",
    "#     tweet_counts = pd.DataFrame(df.groupby(countby_choice).count()[\"message_id\"])\n",
    "#     participation_counts = pd.DataFrame(df.groupby(countby_choice).nunique()[\"screen_name\"])\n",
    "#     users_counts = pd.DataFrame(df.groupby(\"screen_name\").count()[\"message_id\"]\\\n",
    "#     .sort_values(ascending = False)) \n",
    "\n",
    "\n",
    "#     if feature != \"retweets\":\n",
    "#         retweets_df = get_hits_data(\"retweets\",map_id, date_from, date_to)\n",
    "#         retweets_df = retweets_df[[\"message_id\",\"hit_value\"]]\n",
    "#         retweets_df.columns = [\"message_id\", \"retweet_id\"]\n",
    "\n",
    "\n",
    "#         df = pd.merge(results_df,retweets_df,left_on=\"message_id\", right_on=\"message_id\",how = \"left\")\n",
    "\n",
    "#         #save hit values for later join\n",
    "\n",
    "#         hits_retweets = df[[\"retweet_id\",\"hit_value\"]]\n",
    "#         hits_retweets.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "#         df2 = pd.merge(df,nodes,left_on=\"node_id\",right_on=\"node_id\")\n",
    "\n",
    "\n",
    "#         pivot_df = pd.pivot_table(data=df2, columns=countby_choice, values=\"message_id\", aggfunc=pd.Series.nunique, index=\"retweet_id\")\\\n",
    "#             .fillna(0)\n",
    "#         pivot_df[\"map_total\"] = pivot_df.sum(axis=1)\n",
    "#         pivot_df.sort_values(by = \"map_total\", ascending=False, inplace=True)\n",
    "\n",
    "#         pivot_df = pd.merge(pivot_df,hits_retweets,left_index=True, right_on=\"retweet_id\", how=\"left\")\n",
    "\n",
    "#         tweets_json = [tweet for tweet in t.hydrate(list(set(pivot_df.index[0:5000])))]\n",
    "\n",
    "\n",
    "#         all_rows = []\n",
    "#         for tweet in tweets_json:\n",
    "#             row_dict = {}\n",
    "#             row_dict[\"id_str\"] = tweet[\"id_str\"]\n",
    "#             row_dict[\"created_at\"] = tweet[\"created_at\"]\n",
    "#             row_dict[\"full_text\"] = tweet[\"full_text\"]\n",
    "#             row_dict[\"retweet_count\"] = tweet[\"retweet_count\"]\n",
    "#             row_dict[\"favorite_count\"] = tweet[\"favorite_count\"]\n",
    "#             row_dict[\"screen_name\"] = tweet[\"user\"][\"screen_name\"]\n",
    "\n",
    "#             all_rows.append(row_dict)\n",
    "\n",
    "#         tweets_df = pd.DataFrame(all_rows)\n",
    "\n",
    "#         pivot_df.index = pivot_df.index.astype(str)\n",
    "#         top_tweets_df = pd.merge(pivot_df,tweets_df,left_index=True,right_on=\"id_str\")\n",
    "\n",
    "#     writer = pd.ExcelWriter(\"./feature_report_{}.xlsx\".format(map_id))\n",
    "\n",
    "\n",
    "#     feature_count_tweets.to_excel(writer,\"feature_count_tweets\")\n",
    "#     feature_count_part.to_excel(writer,\"feature_count_partic\")\n",
    "#     tweet_counts.to_excel(writer,\"tweet_counts\")\n",
    "#     participation_counts.to_excel(writer, \"partic_counts\")\n",
    "#     users_counts.to_excel(writer,\"users_counts\")\n",
    "#     top_tweets_df.to_excel(writer,\"top_tweets\")\n",
    "\n",
    "#     writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_full_report(debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_activity_report(debug = False):\n",
    "    ##############################\n",
    "    global debug_opt\n",
    "    debug_opt = debug\n",
    "    global map_id\n",
    "    global map_features\n",
    "    global map_feature_data\n",
    "    \n",
    "    input_map_id = input(\">> Enter map id: \")\n",
    "    feature_type = input(\">> Search for hashtags, urls, or retweets: \")\n",
    "    case_sensitive = input(\">> Is this search case sensitive? (y/n) \\n\")\n",
    "    feature_value = (input (\">> Comma separate search parameters, or hit enter for all: \"))\n",
    "    \n",
    "    if case_sensitive == 'n':\n",
    "        case_sensitive = False\n",
    "    else:\n",
    "        case_sensitive = True\n",
    "    feature_value = feature_value.split(\",\")\n",
    "\n",
    "    #Case sensitivity option should be available\n",
    "        \n",
    "    if input_map_id != map_id:\n",
    "        print('...Fetching map data')\n",
    "        map_id = input_map_id\n",
    "        map_features = {'hashtags':pd.DataFrame(),'urls':pd.DataFrame(),'retweets':pd.DataFrame()}\n",
    "    if map_features[feature_type].empty:\n",
    "        print('...Fetching {} data'.format(feature_type))\n",
    "        map_features[feature_type] = get_map_hits(feature_type,map_id)\n",
    "        print('...Fetching {} data'.format('retweet'))\n",
    "        map_features['retweets'] = get_map_hits(feature_type,map_id)\n",
    "    \n",
    "    df = map_features[feature_type].reset_index()\n",
    "    retweets_df = map_features['retweets'].reset_index()\n",
    "    results_df = df\n",
    "\n",
    "    ###################################\n",
    "#     hits_df = get_hits_data(feature, map_id, date_from, date_to)\n",
    "\n",
    "#     if search_type == \"contains\":\n",
    "#         results_df = hits_df[hits_df.hit_value.str.lower().apply(lambda x: any(term in x for term in q))]\n",
    "\n",
    "#     if search_type == \"exact\":\n",
    "#         results_df = hits_df[hits_df.hit_value.str.lower().isin(q)]\n",
    "\n",
    "\n",
    "#     nodes = get_node_data(map_id)\n",
    "#     nodes.node_id = nodes.node_id.astype(str)    \n",
    "\n",
    "#     df = pd.merge(results_df,nodes,left_on=\"node_id\",right_on=\"node_id\")\n",
    "\n",
    "#     df = merge_tags(df)\n",
    "#     return df\n",
    "\n",
    "    choice = input(\">> Please enter which to aggregate the data by – tag or group: \")\n",
    "    choices_dict = {'group':'group_name','tag':'tag'}\n",
    "    groupby_name = choices_dict[choice]\n",
    "    \n",
    "    feature_count_tweets = df.pivot_table(index=\"hit_value\",columns=groupby_name,aggfunc=\"count\",values=\"message_id\").fillna(0)\n",
    "    feature_count_tweets[\"total\"] = feature_count_tweets.sum(axis=1)\n",
    "    feature_count_tweets.sort_values(by=\"total\", ascending=False)\n",
    "\n",
    "    feature_count_part = df.pivot_table(index=\"hit_value\",columns=groupby_name,aggfunc=pd.Series.nunique,values=\"node_id\").fillna(0)\n",
    "    feature_count_part[\"total\"] = feature_count_tweets.sum(axis=1)\n",
    "    feature_count_part.sort_values(by=\"total\", ascending=False)\n",
    "\n",
    "    df.drop_duplicates([\"message_id\"], inplace=True)\n",
    "\n",
    "    tweet_counts = pd.DataFrame(df.groupby(groupby_name).count()[\"message_id\"])\n",
    "    participation_counts = pd.DataFrame(df.groupby(groupby_name).nunique()[\"screen_name\"])\n",
    "    users_counts = pd.DataFrame(df.groupby(\"screen_name\").count()[\"message_id\"]\\\n",
    "    .sort_values(ascending = False)) \n",
    "\n",
    "\n",
    "    if feature_type != \"retweets\":\n",
    "#         retweets_df = get_hits_data(\"retweets\",map_id, date_from, date_to)\n",
    "        retweets_df = retweets_df[[\"message_id\",\"hit_value\"]]\n",
    "        retweets_df.columns = [\"message_id\", \"retweet_id\"]\n",
    "\n",
    "#         df = df.reset_index()\n",
    "#         results_df = results_df.reset_index()\n",
    "        \n",
    "        df = pd.merge(results_df,retweets_df,left_on=\"message_id\", right_on=\"message_id\",how = \"left\")\n",
    "\n",
    "        #save hit values for later join\n",
    "#         display(df)\n",
    "#         df = df.reset_index()\n",
    "#         display(df)\n",
    "        hits_retweets = df[[\"retweet_id\",\"hit_value\"]]\n",
    "        hits_retweets.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "#         df2 = pd.merge(df,nodes,left_on=\"node_id\",right_on=\"node_id\")\n",
    "        df2 = df\n",
    "\n",
    "\n",
    "        pivot_df = pd.pivot_table(data=df2, columns=groupby_name, values=\"message_id\", aggfunc=pd.Series.nunique, index=\"retweet_id\")\\\n",
    "            .fillna(0)\n",
    "        pivot_df[\"map_total\"] = pivot_df.sum(axis=1)\n",
    "        pivot_df.sort_values(by = \"map_total\", ascending=False, inplace=True)\n",
    "\n",
    "        pivot_df = pd.merge(pivot_df,hits_retweets,left_index=True, right_on=\"retweet_id\", how=\"left\")\n",
    "\n",
    "        tweets_json = [tweet for tweet in t.hydrate(list(set(pivot_df.index[0:5000])))]\n",
    "\n",
    "        all_rows = []\n",
    "        for tweet in tweets_json:\n",
    "            row_dict = {}\n",
    "            row_dict[\"id_str\"] = tweet[\"id_str\"]\n",
    "            row_dict[\"created_at\"] = tweet[\"created_at\"]\n",
    "            row_dict[\"full_text\"] = tweet[\"full_text\"]\n",
    "            row_dict[\"retweet_count\"] = tweet[\"retweet_count\"]\n",
    "            row_dict[\"favorite_count\"] = tweet[\"favorite_count\"]\n",
    "            row_dict[\"screen_name\"] = tweet[\"user\"][\"screen_name\"]\n",
    "\n",
    "            all_rows.append(row_dict)\n",
    "\n",
    "        tweets_df = pd.DataFrame(all_rows)\n",
    "\n",
    "        pivot_df.index = pivot_df.index.astype(str)\n",
    "        top_tweets_df = pd.merge(pivot_df,tweets_df,left_index=True,right_on=\"id_str\")\n",
    "\n",
    "    writer = pd.ExcelWriter(\"./feature_report_{}.xlsx\".format(map_id))\n",
    "\n",
    "\n",
    "    feature_count_tweets.to_excel(writer,\"feature_count_tweets\")\n",
    "    feature_count_part.to_excel(writer,\"feature_count_partic\")\n",
    "    tweet_counts.to_excel(writer,\"tweet_counts\")\n",
    "    participation_counts.to_excel(writer, \"partic_counts\")\n",
    "    users_counts.to_excel(writer,\"users_counts\")\n",
    "    top_tweets_df.to_excel(writer,\"top_tweets\")\n",
    "\n",
    "    writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
